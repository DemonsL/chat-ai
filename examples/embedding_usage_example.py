#!/usr/bin/env python3
"""
Embedding æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨ chat-ai é¡¹ç›®ä¸­ä½¿ç”¨ä¸åŒçš„åµŒå…¥æ¨¡å‹æä¾›å•†ï¼ˆOpenAIã€Qwenï¼‰
"""

import asyncio
import os
from typing import List, Dict, Any
from unittest.mock import Mock

# æ¨¡æ‹Ÿå¯¼å…¥ï¼Œå®é™…ä½¿ç”¨æ—¶è¯·ç¡®ä¿æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
try:
    from app.llm.rag.retrieval_service import LLMRetrievalService
    from app.services.retrieval_service import RetrievalService
    from app.db.repositories.user_file_repository import UserFileRepository
except ImportError:
    print("æ³¨æ„ï¼šåœ¨å®é™…é¡¹ç›®ç¯å¢ƒä¸­è¿è¡Œæ­¤ç¤ºä¾‹")


async def example_1_basic_usage():
    """ç¤ºä¾‹1ï¼šåŸºç¡€ä½¿ç”¨ - åˆ›å»ºä¸åŒçš„åµŒå…¥æœåŠ¡"""
    print("=== ç¤ºä¾‹1ï¼šåŸºç¡€ä½¿ç”¨ ===")
    
    # 1. ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºæœåŠ¡ï¼ˆåŸºäºç¯å¢ƒå˜é‡ EMBEDDING_PROVIDERï¼‰
    print("1. åˆ›å»ºé»˜è®¤åµŒå…¥æœåŠ¡...")
    default_service = LLMRetrievalService()
    print(f"   é»˜è®¤æä¾›å•†: {default_service.embedding_provider}")
    
    # 2. æ˜¾å¼åˆ›å»º OpenAI åµŒå…¥æœåŠ¡
    print("2. åˆ›å»º OpenAI åµŒå…¥æœåŠ¡...")
    openai_service = LLMRetrievalService.create_with_provider("openai")
    print(f"   OpenAI æä¾›å•†: {openai_service.embedding_provider}")
    
    # 3. æ˜¾å¼åˆ›å»º Qwen åµŒå…¥æœåŠ¡
    print("3. åˆ›å»º Qwen åµŒå…¥æœåŠ¡...")
    try:
        qwen_service = LLMRetrievalService.create_with_provider("qwen")
        print(f"   Qwen æä¾›å•†: {qwen_service.embedding_provider}")
    except ImportError:
        print("   æ³¨æ„ï¼šéœ€è¦å®‰è£… langchain-community å’Œ dashscope åº“æ‰èƒ½ä½¿ç”¨ Qwen åµŒå…¥")
    
    print()


async def example_2_model_info():
    """ç¤ºä¾‹2ï¼šè·å–æ¨¡å‹ä¿¡æ¯"""
    print("=== ç¤ºä¾‹2ï¼šè·å–æ¨¡å‹ä¿¡æ¯ ===")
    
    # åˆ›å»ºæœåŠ¡å®ä¾‹
    service = LLMRetrievalService(embedding_provider="openai")
    
    # è·å–åµŒå…¥æ¨¡å‹ä¿¡æ¯
    info = service.get_embedding_info()
    print("åµŒå…¥æ¨¡å‹ä¿¡æ¯:")
    print(f"  æä¾›å•†: {info.get('provider')}")
    print(f"  æ¨¡å‹åç§°: {info.get('model_name')}")
    print(f"  å‘é‡ç»´åº¦: {info.get('dimension')}")
    print(f"  æ¨¡å‹ç±»: {info.get('model_class')}")
    
    # æµ‹è¯•è¿æ¥çŠ¶æ€
    print("\næµ‹è¯•è¿æ¥çŠ¶æ€:")
    try:
        connection_result = await service.test_embedding_connection()
        if connection_result.get('success'):
            print(f"  âœ“ è¿æ¥æˆåŠŸ: {connection_result.get('message')}")
        else:
            print(f"  âœ— è¿æ¥å¤±è´¥: {connection_result.get('message')}")
    except Exception as e:
        print(f"  âœ— è¿æ¥æµ‹è¯•å‡ºé”™: {str(e)}")
    
    print()


async def example_3_document_operations():
    """ç¤ºä¾‹3ï¼šæ–‡æ¡£æ“ä½œ - æ·»åŠ å’Œæœç´¢"""
    print("=== ç¤ºä¾‹3ï¼šæ–‡æ¡£æ“ä½œ ===")
    
    # åˆ›å»ºåµŒå…¥æœåŠ¡
    service = LLMRetrievalService(embedding_provider="openai")
    
    # æ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®
    documents = [
        "è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼ŒåŒ…å«å…³äºæœºå™¨å­¦ä¹ çš„å†…å®¹ã€‚",
        "ç¬¬äºŒä¸ªæ–‡æ¡£è®¨è®ºäº†æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œã€‚",
        "ç¬¬ä¸‰ä¸ªæ–‡æ¡£ä»‹ç»äº†è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€‚"
    ]
    
    # æ„å»ºå…ƒæ•°æ®
    metadatas = [
        {"file_id": "file_1", "source": "ml_basics.txt", "chunk_index": 0},
        {"file_id": "file_1", "source": "ml_basics.txt", "chunk_index": 1},
        {"file_id": "file_2", "source": "nlp_guide.txt", "chunk_index": 0}
    ]
    
    print("1. æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨...")
    try:
        success = await service.add_documents_to_vector_store(documents, metadatas)
        if success:
            print("   âœ“ æ–‡æ¡£æ·»åŠ æˆåŠŸ")
        else:
            print("   âœ— æ–‡æ¡£æ·»åŠ å¤±è´¥")
    except Exception as e:
        print(f"   âœ— æ·»åŠ æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
    
    print("2. æœç´¢ç›¸å…³æ–‡æ¡£...")
    try:
        search_results = await service.search_documents(
            query="æ·±åº¦å­¦ä¹ ç›¸å…³å†…å®¹",
            file_ids=["file_1", "file_2"],
            top_k=2,
            similarity_threshold=0.8
        )
        
        print(f"   æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£:")
        for i, result in enumerate(search_results, 1):
            print(f"     {i}. ç›¸ä¼¼åº¦: {result.get('similarity_score', 'N/A'):.4f}")
            print(f"        å†…å®¹é¢„è§ˆ: {result.get('content', '')[:50]}...")
            print(f"        æ¥æº: {result.get('metadata', {}).get('source', 'N/A')}")
    except Exception as e:
        print(f"   âœ— æœç´¢æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
    
    print()


async def example_4_business_layer():
    """ç¤ºä¾‹4ï¼šä¸šåŠ¡å±‚é›†æˆ"""
    print("=== ç¤ºä¾‹4ï¼šä¸šåŠ¡å±‚é›†æˆ ===")
    
    # æ¨¡æ‹Ÿæ–‡ä»¶ä»“åº“
    mock_file_repo = Mock(spec=UserFileRepository)
    
    print("1. åˆ›å»ºä¸šåŠ¡å±‚æ£€ç´¢æœåŠ¡...")
    
    # ä½¿ç”¨é»˜è®¤æä¾›å•†
    default_retrieval = RetrievalService(mock_file_repo)
    print(f"   é»˜è®¤æœåŠ¡æä¾›å•†: {default_retrieval.llm_retrieval.embedding_provider}")
    
    # ä½¿ç”¨æŒ‡å®šæä¾›å•†
    openai_retrieval = RetrievalService.create_with_provider(mock_file_repo, "openai")
    print(f"   OpenAI æœåŠ¡æä¾›å•†: {openai_retrieval.llm_retrieval.embedding_provider}")
    
    print("2. è·å–æ”¯æŒçš„æä¾›å•†åˆ—è¡¨...")
    providers = RetrievalService.get_supported_providers()
    print(f"   æ”¯æŒçš„æä¾›å•†: {', '.join(providers)}")
    
    print("3. æµ‹è¯•åµŒå…¥æ¨¡å‹ä¿¡æ¯è·å–...")
    try:
        info = await default_retrieval.get_embedding_info()
        if "error" not in info:
            print(f"   âœ“ æ¨¡å‹ä¿¡æ¯è·å–æˆåŠŸ: {info.get('provider')}")
        else:
            print(f"   âœ— è·å–å¤±è´¥: {info.get('error')}")
    except Exception as e:
        print(f"   âœ— è·å–æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
    
    print()


async def example_5_provider_comparison():
    """ç¤ºä¾‹5ï¼šæä¾›å•†å¯¹æ¯”"""
    print("=== ç¤ºä¾‹5ï¼šæä¾›å•†å¯¹æ¯” ===")
    
    providers_info = []
    
    for provider in ["openai", "qwen"]:
        try:
            print(f"æ­£åœ¨æµ‹è¯• {provider} æä¾›å•†...")
            service = LLMRetrievalService.create_with_provider(provider)
            info = service.get_embedding_info()
            
            provider_data = {
                "provider": provider,
                "model": info.get('model_name', 'N/A'),
                "dimension": info.get('dimension', 'N/A'),
                "status": "å¯ç”¨"
            }
            
            # æµ‹è¯•è¿æ¥
            try:
                connection_result = await service.test_embedding_connection()
                if not connection_result.get('success'):
                    provider_data["status"] = f"è¿æ¥å¤±è´¥: {connection_result.get('message')}"
            except Exception as e:
                provider_data["status"] = f"è¿æ¥é”™è¯¯: {str(e)}"
            
            providers_info.append(provider_data)
            
        except Exception as e:
            providers_info.append({
                "provider": provider,
                "model": "N/A",
                "dimension": "N/A",
                "status": f"åˆå§‹åŒ–å¤±è´¥: {str(e)}"
            })
    
    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    print("\næä¾›å•†å¯¹æ¯”ç»“æœ:")
    print("-" * 60)
    print(f"{'æä¾›å•†':<10} {'æ¨¡å‹':<20} {'ç»´åº¦':<8} {'çŠ¶æ€':<20}")
    print("-" * 60)
    for info in providers_info:
        print(f"{info['provider']:<10} {info['model']:<20} {info['dimension']:<8} {info['status']:<20}")
    
    print()


def example_6_configuration():
    """ç¤ºä¾‹6ï¼šé…ç½®è¯´æ˜"""
    print("=== ç¤ºä¾‹6ï¼šé…ç½®è¯´æ˜ ===")
    
    print("ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹:")
    print()
    
    print("1. ä½¿ç”¨ OpenAI åµŒå…¥:")
    print("   EMBEDDING_PROVIDER=openai")
    print("   EMBEDDING_MODEL=text-embedding-3-small")
    print("   OPENAI_API_KEY=your-openai-api-key")
    print()
    
    print("2. ä½¿ç”¨ Qwen åµŒå…¥:")
    print("   EMBEDDING_PROVIDER=qwen")
    print("   QWEN_EMBEDDING_MODEL=text-embedding-v1")
    print("   QWEN_API_KEY=your-qwen-api-key")
    print("   QWEN_BASE_URL=https://dashscope.aliyuncs.com/api/v1")
    print()
    
    print("3. å½“å‰ç¯å¢ƒå˜é‡:")
    env_vars = [
        "EMBEDDING_PROVIDER",
        "OPENAI_API_KEY", 
        "QWEN_API_KEY",
        "EMBEDDING_MODEL",
        "QWEN_EMBEDDING_MODEL"
    ]
    
    for var in env_vars:
        value = os.getenv(var, "æœªè®¾ç½®")
        # éšè—APIå¯†é’¥çš„éƒ¨åˆ†å†…å®¹
        if "API_KEY" in var and value != "æœªè®¾ç½®":
            value = value[:8] + "..." if len(value) > 8 else "***"
        print(f"   {var}: {value}")
    
    print()


async def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ Embedding æ¨¡å‹å¤šæä¾›å•†æ”¯æŒç¤ºä¾‹")
    print("=" * 50)
    print()
    
    try:
        await example_1_basic_usage()
        await example_2_model_info()
        await example_3_document_operations()
        await example_4_business_layer()
        await example_5_provider_comparison()
        example_6_configuration()
        
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
        print()
        print("ğŸ’¡ æç¤º:")
        print("   - ç¡®ä¿å·²æ­£ç¡®é…ç½®ç›¸åº”çš„APIå¯†é’¥")
        print("   - Qwen åµŒå…¥éœ€è¦å®‰è£…ä¾èµ–: pip install langchain-community dashscope")
        print("   - å®é™…ä½¿ç”¨æ—¶è¯·åœ¨é¡¹ç›®ç¯å¢ƒä¸­è¿è¡Œ")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œç¤ºä¾‹æ—¶å‡ºé”™: {str(e)}")
        print("è¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main()) 