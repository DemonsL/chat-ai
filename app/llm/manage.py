import json
from typing import AsyncGenerator, Dict, List, Optional, Any, Annotated, Literal, Tuple
from uuid import UUID
from enum import Enum
from loguru import logger

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain.chat_models import init_chat_model
from typing_extensions import TypedDict
from pydantic import BaseModel, Field

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

from app.llm.core.base import (
    StreamingChunk,
    create_chat_model,
    convert_messages_to_langchain,
    stream_chat_model_response
)
from app.llm.core.prompts import prompt_manager
from app.llm.core.checkpointer import get_checkpointer, get_conversation_config
from app.llm.rag.retrieval_service import LLMRetrievalService
from app.llm.rag.file_processor import LLMFileProcessor


from app.core.exceptions import (FileProcessingException,
                                 InvalidFileTypeException)
from langchain_core.documents import Document


class ConversationMode(str, Enum):
    """ä¼šè¯æ¨¡å¼æšä¸¾"""
    CHAT = "chat"
    RAG = "rag"
    AGENT = "agent"
    SEARCH = "search"
    DEEPRESEARCH = "deepresearch"


class DocQARouterResult(BaseModel):
    """DocQAè·¯ç”±å™¨ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹"""
    question_category: Literal["document_related", "non_document"] = Field(
        description="é—®é¢˜å¤§ç±»ï¼šdocument_related(æ–‡æ¡£ç›¸å…³)ã€non_document(éæ–‡æ¡£ç›¸å…³)"
    )
    analysis_type: Literal["full_document", "keyword_search", "general_chat"] = Field(
        description="å…·ä½“åˆ†æç±»å‹ï¼šfull_document(å…¨æ–‡æ¡£åˆ†æ)ã€keyword_search(å…³é”®è¯æ£€ç´¢)ã€general_chat(ä¸€èˆ¬å¯¹è¯)"
    )
    reasoning: str = Field(
        description="åˆ†ææ¨ç†è¿‡ç¨‹å’Œä¾æ®"
    )
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="åˆ†æç»“æœçš„ç½®ä¿¡åº¦ï¼ŒèŒƒå›´0.0-1.0"
    )


class ConversationState(TypedDict):
    """LangGraph å¯¹è¯çŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[BaseMessage], add_messages]
    model_config: Dict[str, Any]
    system_prompt: Optional[str]
    mode: str
    retrieved_documents: Optional[List[str]]
    available_tools: Optional[List[str]]
    metadata: Optional[Dict[str, Any]]
    user_query: Optional[str]
    final_response: Optional[str]
    conversation_id: Optional[str]  # æ·»åŠ å¯¹è¯IDç”¨äºcheckpointer
    # æ·±åº¦ç ”ç©¶ç›¸å…³çŠ¶æ€
    research_iterations: Optional[int]
    search_history: Optional[List[Dict[str, Any]]]
    current_findings: Optional[List[str]]
    research_plan: Optional[str]


class LLMManager:
    """
    åŸºäº LangGraph çš„ LLMç¼–æ’æœåŠ¡
    ä½¿ç”¨çŠ¶æ€å›¾ç®¡ç†å¤šè½®å¯¹è¯æµç¨‹ï¼Œæ”¯æŒ PostgresSaver checkpointer
    """
    
    def __init__(self):
        self._model_cache = {}  # ç¼“å­˜å·²åˆ›å»ºçš„æ¨¡å‹å®ä¾‹
        self._graphs = {}  # ç¼“å­˜ä¸åŒæ¨¡å¼çš„å›¾
        self.retrieval_service = LLMRetrievalService()  # æ£€ç´¢æœåŠ¡ä¾èµ–
        self.file_mgr = LLMFileProcessor()
        
    def _get_model(self, model_config: Dict[str, Any]) -> BaseChatModel:
        """è·å–æˆ–åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = f"{model_config['provider']}-{model_config['model_id']}"
        
        if cache_key not in self._model_cache:
            try:
                # å‡†å¤‡æ¨¡å‹å‚æ•°
                model_params = {
                    "temperature": model_config.get("temperature", 0.7),
                    "max_tokens": model_config.get("max_tokens"),
                }
                
                # æ·»åŠ é¢å¤–å‚æ•°
                if model_config.get("extra_params"):
                    model_params.update(model_config["extra_params"])
                
                # è¿‡æ»¤Noneå€¼
                model_params = {k: v for k, v in model_params.items() if v is not None}
                
                # åˆ›å»ºæ¨¡å‹
                self._model_cache[cache_key] = create_chat_model(
                    provider=model_config["provider"],
                    model=model_config["model_id"],
                    **model_params
                )
            except Exception as e:
                raise ValueError(f"åˆ›å»ºæ¨¡å‹å¤±è´¥ {model_config['provider']}/{model_config['model_id']}: {str(e)}")
        
        return self._model_cache[cache_key]
    
    def _build_chat_graph(self) -> StateGraph:
        """æ„å»ºèŠå¤©æ¨¡å¼çš„çŠ¶æ€å›¾"""
        def chat_node(state: ConversationState) -> Dict[str, Any]:
            """èŠå¤©èŠ‚ç‚¹å¤„ç†å‡½æ•°"""
            model = self._get_model(state["model_config"])
            
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            final_messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤º - ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_chat_prompt()
            final_messages.append(SystemMessage(content=system_prompt))
            
            # å®‰å…¨åœ°æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_messages = state.get("messages") or []
            final_messages.extend(user_messages)
            
            # è°ƒç”¨æ¨¡å‹
            response = model.invoke(final_messages)
            
            return {
                "messages": [response],
                "final_response": response.content,
                "metadata": {"mode": "chat"}
            }
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        graph_builder.add_node("chat", chat_node)
        graph_builder.add_edge(START, "chat")
        graph_builder.add_edge("chat", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_rag_graph(self) -> StateGraph:
        """æ„å»ºRAGæ¨¡å¼çš„çŠ¶æ€å›¾ - åŸºäºDocQA Routerè®¾è®¡"""
        
        def docqa_router_node(state: ConversationState) -> Dict[str, Any]:
            """DocQAè·¯ç”±èŠ‚ç‚¹ - æ™ºèƒ½åˆ¤æ–­é—®é¢˜ç±»å‹"""
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            model = self._get_model(state["model_config"])
            
            # ä½¿ç”¨ChatPromptTemplateæ„å»ºæç¤ºè¯
            router_prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®é¢˜åˆ†æåŠ©æ‰‹ï¼Œéœ€è¦åˆ†æç”¨æˆ·é—®é¢˜å¹¶åˆ†ç±»å¤„ç†ã€‚

è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­å…¶å±äºä»¥ä¸‹å“ªç§ç±»å‹ï¼š

**æ–‡æ¡£ç›¸å…³ç±»å‹ï¼š**
1. **å…¨æ–‡æ¡£åˆ†æ** (full_document)ï¼š
   - è¦æ±‚å¯¹æ–‡æ¡£è¿›è¡Œæ€»ç»“ã€æ¦‚æ‹¬ã€ç»¼è¿°
   - éœ€è¦åˆ†ææ•´ä¸ªæ–‡æ¡£çš„å†…å®¹ç»“æ„  
   - è¦æ±‚æå–æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹ã€ç»“è®º
   - éœ€è¦å¯¹æ–‡æ¡£è¿›è¡Œç¿»è¯‘ã€è½¬æ¢
   - å…³é”®è¯ï¼šæ€»ç»“ã€æ¦‚æ‹¬ã€åˆ†æå…¨æ–‡ã€æ•´ä½“å†…å®¹ã€ä¸»è¦è§‚ç‚¹ã€æ–‡æ¡£æ¦‚è¿°

2. **å…³é”®è¯æ£€ç´¢** (keyword_search)ï¼š
   - è¯¢é—®ç‰¹å®šçš„äº‹å®ã€æ•°æ®ã€æ¦‚å¿µ
   - æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„ç‰¹å®šä¿¡æ¯ç‚¹
   - å›ç­”å…·ä½“é—®é¢˜ï¼Œä¸éœ€è¦å®Œæ•´æ–‡æ¡£
   - å…³é”®è¯ï¼šä»€ä¹ˆæ˜¯ã€å¦‚ä½•ã€ä¸ºä»€ä¹ˆã€å…·ä½“æ•°æ®ã€ç‰¹å®šæ¦‚å¿µ

**éæ–‡æ¡£ç›¸å…³ç±»å‹ï¼š**
3. **ä¸€èˆ¬å¯¹è¯** (general_chat)ï¼š
   - é—®å€™ã€å¯’æš„ã€æ„Ÿè°¢ç­‰ç¤¾äº¤å¯¹è¯
   - å…³äºç³»ç»ŸåŠŸèƒ½çš„è¯¢é—®
   - ä¸éœ€è¦æ–‡æ¡£å†…å®¹çš„å¸¸è¯†æ€§é—®é¢˜
   - é—²èŠã€å¨±ä¹æ€§å¯¹è¯
   - å…³é”®è¯ï¼šä½ å¥½ã€è°¢è°¢ã€å†è§ã€æ€ä¹ˆæ ·ã€èŠå¤©

è¯·å‡†ç¡®åˆ†æå¹¶è¿”å›ç»“æ„åŒ–ç»“æœã€‚"""),
                ("user", "ç”¨æˆ·é—®é¢˜ï¼š{query}")
            ])
            
            try:
                # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
                structured_model = model.with_structured_output(DocQARouterResult)
                
                # è°ƒç”¨æ¨¡å‹è¿›è¡Œè·¯ç”±åˆ¤æ–­
                result: DocQARouterResult = structured_model.invoke(
                    router_prompt.format_messages(query=user_query)
                )
                
                # åˆ¤æ–­å…·ä½“ç±»å‹
                question_category = result.question_category
                analysis_type = result.analysis_type
                is_full_document_analysis = analysis_type == "full_document"
                is_non_document = question_category == "non_document"
                
                logger.info(f"DocQAè·¯ç”±åˆ¤æ–­: {question_category} -> {analysis_type} (ç½®ä¿¡åº¦: {result.confidence})")
                logger.info(f"åˆ¤æ–­ç†ç”±: {result.reasoning}")
                
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "question_category": question_category,
                        "analysis_type": analysis_type,
                        "is_full_document_analysis": is_full_document_analysis,
                        "is_non_document": is_non_document,
                        "routing_confidence": result.confidence,
                        "routing_reasoning": result.reasoning,
                        "routing_completed": True
                    }
                }
                
            except Exception as e:
                logger.error(f"DocQAè·¯ç”±åˆ¤æ–­å¤±è´¥: {str(e)}")
                # è·¯ç”±å¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨å…³é”®è¯æ£€ç´¢
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "question_category": "document_related",
                        "analysis_type": "keyword_search",
                        "is_full_document_analysis": False,
                        "is_non_document": False,
                        "routing_error": str(e),
                        "routing_completed": True
                    }
                }
        
        async def sim_search_node(state: ConversationState) -> Dict[str, Any]:
            """ç›¸ä¼¼åº¦æœç´¢èŠ‚ç‚¹ - ä½¿ç”¨ä¼˜åŒ–åçš„æ£€ç´¢æœåŠ¡è¿›è¡Œå¼‚æ­¥æ£€ç´¢"""
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            metadata = state.get("metadata", {})
            model = self._get_model(state["model_config"])
            
            retrieved_docs = []
            retrieval_info = {}
            no_sim_results = False
            
            # æ£€æŸ¥æ£€ç´¢æœåŠ¡æ˜¯å¦å¯ç”¨
            if not self.retrieval_service or not self.retrieval_service.is_ready:
                no_sim_results = True
                retrieval_info = {
                    "method": "langchain_similarity_search",
                    "status": "æ£€ç´¢æœåŠ¡ä¸å¯ç”¨",
                    "query": user_query,
                    "no_sim_results": True
                }
                logger.warning("ç›¸ä¼¼åº¦æœç´¢ï¼šæ£€ç´¢æœåŠ¡ä¸å¯ç”¨")
                
                return {
                    "retrieved_documents": retrieved_docs,
                    "metadata": {
                        **state.get("metadata", {}),
                        "sim_search_completed": True,
                        "no_sim_results": no_sim_results,
                        "retrieval_info": retrieval_info,
                        "document_count": 0
                    }
                }
            
            # æƒé™éªŒè¯ï¼šç¡®ä¿å¿…è¦çš„å®‰å…¨å‚æ•°å­˜åœ¨
            user_id = metadata.get("user_id")
            if not user_id:
                logger.error("ç›¸ä¼¼åº¦æœç´¢ï¼šç¼ºå°‘ç”¨æˆ·IDï¼Œå­˜åœ¨å®‰å…¨é£é™©")
                no_sim_results = True
                retrieval_info = {
                    "method": "langchain_similarity_search",
                    "status": "æƒé™éªŒè¯å¤±è´¥ï¼šç¼ºå°‘ç”¨æˆ·ID",
                    "query": user_query,
                    "no_sim_results": True,
                    "security_error": True
                }
                
                return {
                    "retrieved_documents": retrieved_docs,
                    "metadata": {
                        **state.get("metadata", {}),
                        "sim_search_completed": True,
                        "no_sim_results": no_sim_results,
                        "retrieval_info": retrieval_info,
                        "document_count": 0
                    }
                }
            
            try:
                # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨æ¨¡å‹ä¼˜åŒ–æŸ¥è¯¢
                query_optimization_prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŸ¥è¯¢ä¼˜åŒ–åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„åŸå§‹é—®é¢˜è½¬æ¢ä¸ºæ›´é€‚åˆå‘é‡æ£€ç´¢çš„æŸ¥è¯¢ã€‚

ä¼˜åŒ–åŸåˆ™ï¼š
1. æå–æ ¸å¿ƒå…³é”®è¯å’Œæ¦‚å¿µ
2. å»é™¤å†—ä½™çš„è¯­è¨€è¡¨è¾¾
3. å¢åŠ ç›¸å…³çš„åŒä¹‰è¯å’Œæ¦‚å¿µ
4. ä¿æŒæŸ¥è¯¢çš„è¯­ä¹‰å®Œæ•´æ€§
5. é’ˆå¯¹æ–‡æ¡£æ£€ç´¢è¿›è¡Œä¼˜åŒ–

è¯·è¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼Œä¿æŒç®€æ´ä½†ä¿¡æ¯ä¸°å¯Œã€‚"""),
                    ("user", "åŸå§‹é—®é¢˜: {original_query}")
                ])
                
                logger.info(f"å¼€å§‹ä¼˜åŒ–æŸ¥è¯¢: '{user_query}'")
                
                # è°ƒç”¨æ¨¡å‹ä¼˜åŒ–æŸ¥è¯¢
                optimized_response = model.invoke(
                    query_optimization_prompt.format_messages(original_query=user_query)
                )
                optimized_query = optimized_response.content.strip()
                
                logger.info(f"æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ: '{user_query}' -> '{optimized_query}'")
                
                # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨æ£€ç´¢æœåŠ¡è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
                # ä»å…ƒæ•°æ®ä¸­è·å–ç”¨æˆ·å’Œæ–‡ä»¶ä¿¡æ¯
                available_file_ids = metadata.get("available_file_ids", [])
                conversation_id = state.get("conversation_id")
                
                # éªŒè¯æ–‡ä»¶æƒé™ï¼šç¡®ä¿ç”¨æˆ·æœ‰æƒè®¿é—®æŒ‡å®šçš„æ–‡ä»¶
                if available_file_ids:
                    logger.info(f"æƒé™éªŒè¯ï¼šç”¨æˆ· {user_id} å°è¯•è®¿é—®æ–‡ä»¶ {available_file_ids}")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„æƒé™æ£€æŸ¥é€»è¾‘
                    # ä¾‹å¦‚ï¼šæŸ¥è¯¢æ•°æ®åº“éªŒè¯ç”¨æˆ·å¯¹è¿™äº›æ–‡ä»¶çš„è®¿é—®æƒé™
                
                search_results = await self.retrieval_service.similarity_search_with_score(
                    query=optimized_query,
                    k=5,
                    user_id=user_id,
                    file_ids=available_file_ids if available_file_ids else None,
                    conversation_id=conversation_id
                )
                
                # å¤„ç†æ£€ç´¢ç»“æœ
                similarity_threshold = 0.3  # ç›¸ä¼¼åº¦é˜ˆå€¼
                filtered_results = [
                    (doc, score) for doc, score in search_results 
                    if score >= similarity_threshold
                ]
                
                if filtered_results:
                    retrieved_docs = [doc.page_content for doc, score in filtered_results]
                    scores = [score for doc, score in filtered_results]
                    
                    # å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯è¿”å›çš„æ–‡æ¡£ç¡®å®å±äºå½“å‰ç”¨æˆ·
                    security_validated = True
                    for doc, _ in filtered_results:
                        doc_user_id = doc.metadata.get("user_id")
                        if doc_user_id and doc_user_id != user_id:
                            logger.error(f"å®‰å…¨è­¦å‘Šï¼šæ£€ç´¢åˆ°å…¶ä»–ç”¨æˆ·çš„æ–‡æ¡£ï¼doc_user_id={doc_user_id}, current_user_id={user_id}")
                            security_validated = False
                            break
                    
                    if not security_validated:
                        # å‘ç°å®‰å…¨é—®é¢˜ï¼Œæ¸…ç©ºç»“æœ
                        retrieved_docs = []
                        no_sim_results = True
                        retrieval_info = {
                            "method": "langchain_similarity_search",
                            "status": "å®‰å…¨éªŒè¯å¤±è´¥ï¼šæ£€æµ‹åˆ°è·¨ç”¨æˆ·è®¿é—®",
                            "security_error": True,
                            "no_sim_results": True
                        }
                    else:
                        retrieval_info = {
                            "method": "langchain_similarity_search",
                            "original_query": user_query,
                            "optimized_query": optimized_query,
                            "document_count": len(retrieved_docs),
                            "similarity_scores": scores,
                            "similarity_threshold": similarity_threshold,
                            "total_candidates": len(search_results),
                            "status": "ç›¸ä¼¼åº¦æœç´¢æˆåŠŸ",
                            "security_validated": True
                        }
                        
                        logger.info(f"ç›¸ä¼¼åº¦æœç´¢æˆåŠŸ: æ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                    
                else:
                    # æ²¡æœ‰æ»¡è¶³é˜ˆå€¼çš„ç»“æœ
                    no_sim_results = True
                    all_scores = [score for doc, score in search_results] if search_results else []
                    
                    retrieval_info = {
                        "method": "langchain_similarity_search", 
                        "original_query": user_query,
                        "optimized_query": optimized_query,
                        "document_count": 0,
                        "similarity_threshold": similarity_threshold,
                        "total_candidates": len(search_results),
                        "max_score": max(all_scores) if all_scores else 0,
                        "status": "æ— æ»¡è¶³é˜ˆå€¼çš„ç›¸ä¼¼æ–‡æ¡£",
                        "no_sim_results": True
                    }
                    
                    logger.info(f"ç›¸ä¼¼åº¦æœç´¢: æ— æ»¡è¶³é˜ˆå€¼ {similarity_threshold} çš„æ–‡æ¡£")
                
            except Exception as e:
                logger.error(f"ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {str(e)}")
                no_sim_results = True
                retrieval_info = {
                    "method": "langchain_similarity_search",
                    "original_query": user_query,
                    "optimized_query": optimized_query if 'optimized_query' in locals() else user_query,
                    "error": str(e),
                    "status": "ç›¸ä¼¼åº¦æœç´¢å¤±è´¥",
                    "no_sim_results": True
                }
            
            return {
                "retrieved_documents": retrieved_docs,
                "metadata": {
                    **state.get("metadata", {}),
                    "sim_search_completed": True,
                    "no_sim_results": no_sim_results,
                    "retrieval_info": retrieval_info,
                    "document_count": len(retrieved_docs)
                }
            }
        
        async def full_doc_qa_node(state: ConversationState) -> Dict[str, Any]:
            """å…¨æ–‡æ¡£QAèŠ‚ç‚¹ - è·å–æ›´å¤šæ–‡æ¡£å†…å®¹è¿›è¡Œå…¨æ–‡åˆ†æ"""
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            metadata = state.get("metadata", {})
            
            full_documents = []
            retrieval_info = {}
            
            # æ£€æŸ¥æ£€ç´¢æœåŠ¡æ˜¯å¦å¯ç”¨
            if not self.retrieval_service or not self.retrieval_service.is_ready:
                retrieval_info = {
                    "method": "full_document_retrieval",
                    "status": "æ£€ç´¢æœåŠ¡ä¸å¯ç”¨",
                    "query": user_query
                }
                logger.warning("å…¨æ–‡æ¡£QAï¼šæ£€ç´¢æœåŠ¡ä¸å¯ç”¨")
            else:
                try:
                    logger.info("å¼€å§‹å…¨æ–‡æ¡£å†…å®¹è·å–")
                    
                    # ä½¿ç”¨æ›´å¤§çš„kå€¼å’Œæ›´ä½çš„é˜ˆå€¼è·å–æ›´å¤šæ–‡æ¡£å†…å®¹
                    # ä»å…ƒæ•°æ®ä¸­è·å–ç”¨æˆ·å’Œæ–‡ä»¶ä¿¡æ¯
                    user_id = metadata.get("user_id")
                    available_file_ids = metadata.get("available_file_ids", [])
                    conversation_id = state.get("conversation_id")
                    
                    search_results = await self.retrieval_service.similarity_search_with_score(
                        query=user_query,
                        k=20,  # è·å–æ›´å¤šæ–‡æ¡£ç‰‡æ®µ
                        user_id=user_id,
                        file_ids=available_file_ids if available_file_ids else None,
                        conversation_id=conversation_id
                    )
                    
                    # å¯¹äºå…¨æ–‡æ¡£åˆ†æï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
                    full_doc_threshold = 0.1  # å¾ˆä½çš„é˜ˆå€¼ï¼Œè·å–æ›´å¤šå†…å®¹
                    filtered_results = [
                        (doc, score) for doc, score in search_results 
                        if score >= full_doc_threshold
                    ]
                    
                    if filtered_results:
                        full_documents = [doc.page_content for doc, score in filtered_results]
                        scores = [score for doc, score in filtered_results]
                        
                        retrieval_info = {
                            "method": "full_document_retrieval",
                            "query": user_query,
                            "document_count": len(full_documents),
                            "similarity_scores": scores,
                            "similarity_threshold": full_doc_threshold,
                            "total_candidates": len(search_results),
                            "status": "å…¨æ–‡æ¡£è·å–æˆåŠŸ"
                        }
                        
                        logger.info(f"å…¨æ–‡æ¡£è·å–æˆåŠŸ: è·å¾— {len(full_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                    else:
                        # å¦‚æœè¿ä½é˜ˆå€¼éƒ½æ²¡æœ‰ç»“æœï¼Œåˆ™è·å–æ‰€æœ‰å€™é€‰ç»“æœ
                        if search_results:
                            full_documents = [doc.page_content for doc, score in search_results]
                            scores = [score for doc, score in search_results]
                            
                            retrieval_info = {
                                "method": "full_document_retrieval",
                                "query": user_query,
                                "document_count": len(full_documents),
                                "similarity_scores": scores,
                                "similarity_threshold": "all_candidates",
                                "total_candidates": len(search_results),
                                "status": "å…¨æ–‡æ¡£è·å–æˆåŠŸï¼ˆæ‰€æœ‰å€™é€‰ï¼‰"
                            }
                            
                            logger.info(f"å…¨æ–‡æ¡£è·å–æˆåŠŸï¼ˆæ‰€æœ‰å€™é€‰ï¼‰: è·å¾— {len(full_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                        else:
                            retrieval_info = {
                                "method": "full_document_retrieval",
                                "query": user_query,
                                "document_count": 0,
                                "status": "æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹"
                            }
                            logger.warning("å…¨æ–‡æ¡£è·å–: æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹")
                    
                except Exception as e:
                    logger.error(f"å…¨æ–‡æ¡£è·å–å¤±è´¥: {str(e)}")
                    retrieval_info = {
                        "method": "full_document_retrieval",
                        "error": str(e),
                        "status": "å…¨æ–‡æ¡£è·å–å¤±è´¥",
                        "query": user_query
                    }
            
            return {
                "retrieved_documents": full_documents,
                "metadata": {
                    **state.get("metadata", {}),
                    "full_doc_completed": True,
                    "retrieval_info": retrieval_info,
                    "document_count": len(full_documents),
                    "processing_type": "å…¨æ–‡æ¡£åˆ†æ"
                }
            }
        
        def unified_response_node(state: ConversationState) -> Dict[str, Any]:
            """ç»Ÿä¸€å“åº”èŠ‚ç‚¹ - å¤„ç†æ‰€æœ‰ç±»å‹çš„é—®é¢˜å›ç­”"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            retrieved_docs = state.get("retrieved_documents") or []
            metadata = state.get("metadata", {})
            
            analysis_type = metadata.get("analysis_type", "keyword_search")
            question_category = metadata.get("question_category", "document_related")
            is_full_document_analysis = metadata.get("is_full_document_analysis", False)
            is_non_document = metadata.get("is_non_document", False)
            processing_type = metadata.get("processing_type", "çŸ¥è¯†æ£€ç´¢")
            
            # æ ¹æ®é—®é¢˜ç±»å‹æ„å»ºä¸åŒçš„æç¤ºè¯
            available_file_ids = metadata.get("available_file_ids", [])
            
            if is_non_document:
                # éæ–‡æ¡£ç›¸å…³é—®é¢˜ - ç›´æ¥å¯¹è¯
                response_prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜ä¸éœ€è¦æ–‡æ¡£å†…å®¹æ”¯æŒï¼Œè¯·ç›´æ¥åŸºäºä½ çš„çŸ¥è¯†åº“å›ç­”ã€‚

å¯¹äºä»¥ä¸‹ç±»å‹çš„é—®é¢˜ï¼Œè¯·æä¾›ç›¸åº”çš„å›ç­”ï¼š
- é—®å€™å’Œå¯’æš„ï¼šå‹å¥½å›åº”
- åŠŸèƒ½è¯¢é—®ï¼šç®€è¦è¯´æ˜ç³»ç»ŸåŠŸèƒ½  
- å¸¸è¯†é—®é¢˜ï¼šåŸºäºé€šç”¨çŸ¥è¯†å›ç­”
- é—²èŠå¯¹è¯ï¼šè‡ªç„¶äº’åŠ¨

è¯·ä¿æŒå›ç­”ç®€æ´ã€å‡†ç¡®ã€å‹å¥½ã€‚"""),
                    ("placeholder", "{messages}")
                ])
                context_instruction = None
                
            elif not available_file_ids and not retrieved_docs:
                # æ²¡æœ‰å¯ç”¨æ–‡ä»¶ä¸”é—®é¢˜æ˜¯æ–‡æ¡£ç›¸å…³çš„
                response_prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹ã€‚ç”¨æˆ·æå‡ºäº†ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜ï¼Œä½†å½“å‰å¯¹è¯ä¸­æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£ã€‚

è¯·å‹å¥½åœ°æé†’ç”¨æˆ·ï¼š
1. éœ€è¦å…ˆä¸Šä¼ ç›¸å…³æ–‡æ¡£æ‰èƒ½è¿›è¡Œæ–‡æ¡£åˆ†æ
2. æ”¯æŒçš„æ–‡ä»¶æ ¼å¼åŒ…æ‹¬ï¼šPDFã€DOCXã€TXTç­‰
3. ä¸Šä¼ æ–‡æ¡£åï¼Œæ‚¨å°±å¯ä»¥å¸®åŠ©ç”¨æˆ·åˆ†æã€æ€»ç»“å’Œå›ç­”æ–‡æ¡£ç›¸å…³çš„é—®é¢˜

è¯·ç”¨æ¸©å’Œã€æœ‰å¸®åŠ©çš„è¯­æ°”å›åº”ã€‚"""),
                    ("placeholder", "{messages}")
                ])
                context_instruction = None
                
            elif is_full_document_analysis:
                # å…¨æ–‡æ¡£åˆ†æ
                response_prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚å½“å‰ä»»åŠ¡ç±»å‹ï¼šå…¨æ–‡æ¡£åˆ†æ

è¯·åŸºäºæä¾›çš„å®Œæ•´æ–‡æ¡£å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æã€æ€»ç»“æˆ–å¤„ç†ã€‚
æ³¨æ„æ•´ä½“æ€§å’Œå…¨é¢æ€§ï¼Œæä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœã€‚

{context_instruction}"""),
                    ("placeholder", "{messages}")
                ])
                
                # å‡†å¤‡å…¨æ–‡æ¡£ä¸Šä¸‹æ–‡
                if retrieved_docs:
                    context = "\n\n=== æ–‡æ¡£åˆ†éš”ç¬¦ ===\n\n".join(retrieved_docs)
                    context_instruction = f"å®Œæ•´æ–‡æ¡£å†…å®¹ï¼š\n\n{context}"
                else:
                    context_instruction = "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œè¯·åŸºäºå¸¸è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                    
            else:
                # å…³é”®è¯æ£€ç´¢
                response_prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚å½“å‰ä»»åŠ¡ç±»å‹ï¼šå…³é”®è¯æ£€ç´¢

è¯·åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µå›ç­”ç”¨æˆ·çš„å…·ä½“é—®é¢˜ã€‚
é‡ç‚¹å…³æ³¨é—®é¢˜çš„å‡†ç¡®å›ç­”ï¼Œå¼•ç”¨ç›¸å…³ç‰‡æ®µæ”¯æŒç­”æ¡ˆã€‚

{context_instruction}"""),
                    ("placeholder", "{messages}")
                ])
                
                # å‡†å¤‡å…³é”®è¯æ£€ç´¢ä¸Šä¸‹æ–‡
                if retrieved_docs:
                    context = "\n\n".join([f"ç›¸å…³ç‰‡æ®µ {i+1}:\n{doc}" for i, doc in enumerate(retrieved_docs)])
                    context_instruction = f"æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š\n\n{context}"
                else:
                    context_instruction = "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œè¯·åŸºäºå¸¸è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            
            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
            try:
                if is_non_document:
                    # éæ–‡æ¡£ç›¸å…³é—®é¢˜ç›´æ¥è°ƒç”¨
                    response = model.invoke(response_prompt.format_messages(messages=messages))
                    processing_type = "ç›´æ¥å¯¹è¯"
                    retrieval_method = "none"
                else:
                    # æ–‡æ¡£ç›¸å…³é—®é¢˜éœ€è¦ä¸Šä¸‹æ–‡
                    response = model.invoke(response_prompt.format_messages(
                        context_instruction=context_instruction,
                        messages=messages
                    ))
                    retrieval_method = "full_document" if is_full_document_analysis else "similarity_search"
                
                # å‡†å¤‡æ¥æºä¿¡æ¯
                sources = []
                if retrieved_docs and not is_non_document:
                    sources = [
                        {
                            "content": doc[:200] + "..." if len(doc) > 200 else doc,
                            "index": i,
                            "type": "full_document" if is_full_document_analysis else "similarity_chunk"
                        } 
                        for i, doc in enumerate(retrieved_docs)
                    ]
                
                # å‡†å¤‡å“åº”å…ƒæ•°æ®
                response_metadata = {
                    "mode": "rag",
                    "question_category": question_category,
                    "analysis_type": analysis_type,
                    "is_full_document_analysis": is_full_document_analysis,
                    "is_non_document": is_non_document,
                    "processing_type": processing_type,
                    "document_count": len(retrieved_docs) if not is_non_document else 0,
                    "sources": sources,
                    "retrieval_method": retrieval_method
                }
                
                # æ·»åŠ æ£€ç´¢ä¿¡æ¯
                if "retrieval_info" in metadata and not is_non_document:
                    response_metadata["retrieval_info"] = metadata["retrieval_info"]
                
                logger.info(f"ç»Ÿä¸€å“åº”ç”ŸæˆæˆåŠŸï¼Œç±»å‹: {question_category} -> {analysis_type}, æ–‡æ¡£æ•°: {len(retrieved_docs) if not is_non_document else 0}")
                
                return {
                    "messages": [response],
                    "final_response": response.content,
                    "metadata": response_metadata
                }
                
            except Exception as e:
                logger.error(f"ç»Ÿä¸€å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
                return {
                    "final_response": f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}",
                    "metadata": {
                        "mode": "rag",
                        "error": True,
                        "error_type": "unified_response_failed",
                        "error_message": str(e),
                        "analysis_type": analysis_type,
                        "question_category": question_category
                    }
                }
        
        def route_after_router(state: ConversationState) -> str:
            """è·¯ç”±å™¨åçš„æ¡ä»¶è·¯ç”± - æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©å¤„ç†æµç¨‹"""
            metadata = state.get("metadata", {})
            is_non_document = metadata.get("is_non_document", False)
            is_full_document_analysis = metadata.get("is_full_document_analysis", False)
            available_file_ids = metadata.get("available_file_ids", [])
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            logger.info(f"è·¯ç”±å†³ç­–: is_non_document={is_non_document}, is_full_document_analysis={is_full_document_analysis}, available_file_ids={available_file_ids}")
            
            # å¦‚æœæ˜¯éæ–‡æ¡£ç›¸å…³é—®é¢˜ï¼Œç›´æ¥å“åº”
            if is_non_document:
                logger.info("åˆ¤æ–­ä¸ºéæ–‡æ¡£ç›¸å…³é—®é¢˜ï¼Œè¿›å…¥ç»Ÿä¸€å“åº”æµç¨‹")
                return "unified_response"
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨æ–‡ä»¶ï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼ˆæé†’ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶ï¼‰
            if not available_file_ids:
                logger.info("æ²¡æœ‰å¯ç”¨æ–‡ä»¶ï¼Œè¿›å…¥ç»Ÿä¸€å“åº”æµç¨‹")
                return "unified_response"
            
            # å¦‚æœæ˜¯å…¨æ–‡æ¡£åˆ†æï¼Œç›´æ¥è¿›å…¥å…¨æ–‡æ¡£QAèŠ‚ç‚¹
            if is_full_document_analysis:
                logger.info("åˆ¤æ–­ä¸ºå…¨æ–‡æ¡£åˆ†æï¼Œè¿›å…¥å…¨æ–‡æ¡£QAæµç¨‹")
                return "full_doc_qa"
            
            # å¦åˆ™è¿›å…¥ç›¸ä¼¼åº¦æœç´¢
            logger.info("åˆ¤æ–­ä¸ºå…³é”®è¯æ£€ç´¢ï¼Œè¿›å…¥ç›¸ä¼¼åº¦æœç´¢æµç¨‹")
            return "sim_search"
        
        def route_after_sim_search(state: ConversationState) -> str:
            """ç›¸ä¼¼åº¦æœç´¢åçš„æ¡ä»¶è·¯ç”± - åˆ¤æ–­NoSim"""
            metadata = state.get("metadata", {})
            no_sim_results = metadata.get("no_sim_results", False)
            
            # å¦‚æœæ²¡æœ‰ç›¸ä¼¼ç»“æœï¼Œè¿›å…¥å…¨æ–‡æ¡£QAèŠ‚ç‚¹
            if no_sim_results:
                logger.info("ç›¸ä¼¼åº¦æœç´¢æ— ç»“æœï¼Œè½¬å…¥å…¨æ–‡æ¡£QAæµç¨‹")
                return "full_doc_qa"
            
            # æœ‰ç›¸ä¼¼ç»“æœï¼Œè¿›å…¥ç»Ÿä¸€å“åº”ç”Ÿæˆ
            logger.info("ç›¸ä¼¼åº¦æœç´¢æœ‰ç»“æœï¼Œè¿›å…¥ç»Ÿä¸€å“åº”ç”Ÿæˆ")
            return "unified_response"
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("docqa_router", docqa_router_node)
        graph_builder.add_node("sim_search", sim_search_node)  # å¼‚æ­¥èŠ‚ç‚¹
        graph_builder.add_node("full_doc_qa", full_doc_qa_node)
        graph_builder.add_node("unified_response", unified_response_node)
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
        graph_builder.add_edge(START, "docqa_router")
        
        # DocQA Router åçš„æ¡ä»¶è·¯ç”±
        graph_builder.add_conditional_edges(
            "docqa_router",
            route_after_router,
            {
                "unified_response": "unified_response",
                "sim_search": "sim_search", 
                "full_doc_qa": "full_doc_qa"
            }
        )
        
        # ç›¸ä¼¼åº¦æœç´¢åçš„æ¡ä»¶è·¯ç”± (åˆ¤æ–­NoSim)
        graph_builder.add_conditional_edges(
            "sim_search",
            route_after_sim_search,
            {
                "full_doc_qa": "full_doc_qa",
                "unified_response": "unified_response"
            }
        )
        
        # å…¨æ–‡æ¡£QAåè¿›å…¥ç»Ÿä¸€å“åº”
        graph_builder.add_edge("full_doc_qa", "unified_response")
        
        # ç»Ÿä¸€å“åº”å®Œæˆ
        graph_builder.add_edge("unified_response", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_agent_graph(self) -> StateGraph:
        """æ„å»ºAgentæ¨¡å¼çš„çŠ¶æ€å›¾"""
        def agent_planning_node(state: ConversationState) -> Dict[str, Any]:
            """Agentè§„åˆ’èŠ‚ç‚¹"""
            tools = state.get("available_tools", [])
            
            # æ·»åŠ ç³»ç»Ÿæç¤º - ä½¿ç”¨Agentä¸“ç”¨æç¤ºè¯
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_agent_prompt(available_tools=tools)
            
            planning_messages = [SystemMessage(content=system_prompt)]
            
            # å®‰å…¨åœ°æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_messages = state.get("messages") or []
            planning_messages.extend(user_messages)
            
            return {
                "messages": planning_messages,
                "metadata": {"planning_completed": True, "available_tools": tools}
            }
        
        def agent_response_node(state: ConversationState) -> Dict[str, Any]:
            """Agentå“åº”èŠ‚ç‚¹"""
            model = self._get_model(state["model_config"])
            
            # å®‰å…¨åœ°è·å–æ¶ˆæ¯
            messages = state.get("messages") or []
            if not messages:
                # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼Œè¿”å›é”™è¯¯
                return {
                    "final_response": "æ²¡æœ‰å¯å¤„ç†çš„æ¶ˆæ¯",
                    "metadata": {
                        "mode": "agent",
                        "error": True,
                        "error_type": "no_messages",
                        "error_message": "æ²¡æœ‰å¯å¤„ç†çš„æ¶ˆæ¯"
                    }
                }
            
            # è°ƒç”¨æ¨¡å‹
            response = model.invoke(messages)
            
            return {
                "messages": [response],
                "final_response": response.content,
                "metadata": {"mode": "agent", "is_tool_use": False}
            }
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        graph_builder.add_node("agent_planning", agent_planning_node)
        graph_builder.add_node("agent_response", agent_response_node)
        
        graph_builder.add_edge(START, "agent_planning")
        graph_builder.add_edge("agent_planning", "agent_response")
        graph_builder.add_edge("agent_response", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_search_graph(self) -> StateGraph:
        """æ„å»ºæœç´¢æ¨¡å¼çš„çŠ¶æ€å›¾"""
        
        def search_planning_node(state: ConversationState) -> Dict[str, Any]:
            """æœç´¢è§„åˆ’èŠ‚ç‚¹ - åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            
            # æ„å»ºæœç´¢è§„åˆ’æç¤º
            planning_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œå¹¶ç”Ÿæˆ1-3ä¸ªç›¸å…³ä½†ä¸é‡å¤çš„æœç´¢æŸ¥è¯¢æ¥è·å–å…¨é¢ä¿¡æ¯ã€‚
æ¯ä¸ªæœç´¢æŸ¥è¯¢åº”è¯¥ä»ä¸åŒè§’åº¦æˆ–æ–¹é¢æ¥æ¢ç´¢è¿™ä¸ªé—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

è¯·åªè¿”å›æœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šï¼š
"""
            
            planning_messages = [SystemMessage(content=planning_prompt)]
            
            try:
                response = model.invoke(planning_messages)
                search_queries_text = response.content.strip()
                
                # è§£ææœç´¢æŸ¥è¯¢
                search_queries = []
                for line in search_queries_text.split('\n'):
                    query = line.strip()
                    if query and not query.startswith('#') and not query.startswith('æœç´¢æŸ¥è¯¢'):
                        # æ¸…ç†å¯èƒ½çš„åºå·æˆ–æ ‡ç‚¹
                        import re
                        query = re.sub(r'^\d+[\.ã€]\s*', '', query)
                        query = query.strip('- ')
                        if query:
                            search_queries.append(query)
                
                # é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡
                search_queries = search_queries[:3]
                
                if not search_queries:
                    search_queries = [user_query]  # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                
                logger.info(f"æœç´¢è§„åˆ’å®Œæˆï¼Œç”Ÿæˆ {len(search_queries)} ä¸ªæŸ¥è¯¢: {search_queries}")
                
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_queries": search_queries,
                        "planning_completed": True
                    }
                }
                
            except Exception as e:
                logger.error(f"æœç´¢è§„åˆ’å¤±è´¥: {str(e)}")
                # è§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_queries": [user_query],
                        "planning_completed": True,
                        "planning_error": str(e)
                    }
                }
        
        def execute_search_node(state: ConversationState) -> Dict[str, Any]:
            """æ‰§è¡Œæœç´¢èŠ‚ç‚¹ - ä½¿ç”¨DuckDuckGoè¿›è¡Œå®é™…æœç´¢"""
            import asyncio
            from app.llm.tools.duckduckgo_search import duckduckgo_search_tool
            
            metadata = state.get("metadata", {})
            search_queries = metadata.get("search_queries", [])
            
            search_results = []
            search_info = {}
            
            if not search_queries:
                search_info = {
                    "status": "æœç´¢å¤±è´¥",
                    "error": "æ²¡æœ‰ç”Ÿæˆæœç´¢æŸ¥è¯¢",
                    "query_count": 0,
                    "result_count": 0
                }
            else:
                try:
                    logger.info(f"å¼€å§‹æ‰§è¡Œ {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
                    
                    # ä½¿ç”¨DuckDuckGoå·¥å…·è¿›è¡Œæœç´¢
                    for i, query in enumerate(search_queries):
                        try:
                            # è°ƒç”¨DuckDuckGoæœç´¢å·¥å…·
                            result = duckduckgo_search_tool.invoke({"query": query})
                            
                            if result and isinstance(result, str):
                                # æ·»åŠ æœç´¢ç»“æœï¼Œæ ‡æ˜æ¥æºæŸ¥è¯¢
                                search_results.append({
                                    "query": query,
                                    "content": result,
                                    "source": f"æœç´¢æŸ¥è¯¢ {i+1}",
                                    "tool": "duckduckgo"
                                })
                                logger.info(f"æŸ¥è¯¢ '{query}' æœç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(result)}")
                            else:
                                logger.warning(f"æŸ¥è¯¢ '{query}' æ²¡æœ‰è¿”å›æœ‰æ•ˆç»“æœ")
                        
                        except Exception as e:
                            logger.error(f"æŸ¥è¯¢ '{query}' æœç´¢å¤±è´¥: {str(e)}")
                            search_results.append({
                                "query": query,
                                "content": f"æœç´¢å¤±è´¥: {str(e)}",
                                "source": f"æœç´¢æŸ¥è¯¢ {i+1}",
                                "tool": "duckduckgo",
                                "error": True
                            })
                    
                    search_info = {
                        "status": "æœç´¢å®Œæˆ",
                        "query_count": len(search_queries),
                        "result_count": len([r for r in search_results if not r.get("error", False)]),
                        "error_count": len([r for r in search_results if r.get("error", False)]),
                        "queries": search_queries
                    }
                    
                    logger.info(f"æœç´¢æ‰§è¡Œå®Œæˆ: æˆåŠŸ {search_info['result_count']} ä¸ªï¼Œå¤±è´¥ {search_info['error_count']} ä¸ª")
                    
                except Exception as e:
                    logger.error(f"æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
                    search_info = {
                        "status": "æœç´¢å¤±è´¥",
                        "error": str(e),
                        "query_count": len(search_queries),
                        "result_count": 0
                    }
            
            return {
                "retrieved_documents": [r["content"] for r in search_results if not r.get("error", False)],
                "metadata": {
                    **state.get("metadata", {}),
                    "search_completed": True,
                    "search_results": search_results,
                    "search_info": search_info
                }
            }
        
        def search_response_node(state: ConversationState) -> Dict[str, Any]:
            """æœç´¢å“åº”èŠ‚ç‚¹ - åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­”"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            retrieved_docs = state.get("retrieved_documents") or []
            metadata = state.get("metadata", {})
            
            # è·å–æœç´¢ç›¸å…³ä¿¡æ¯
            search_info = metadata.get("search_info", {})
            search_results = metadata.get("search_results", [])
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_search_prompt()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            final_messages = [SystemMessage(content=system_prompt)]
            
            # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œæ·»åŠ æœç´¢ä¸Šä¸‹æ–‡
            if retrieved_docs:
                context_parts = []
                for i, (doc, result) in enumerate(zip(retrieved_docs, search_results)):
                    if not result.get("error", False):
                        context_parts.append(f"## æœç´¢ç»“æœ {i+1}: {result['query']}\n\n{doc}")
                
                if context_parts:
                    search_context = "\n\n---\n\n".join(context_parts)
                    context_message = f"åŸºäºä»¥ä¸‹æœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n{search_context}"
                    final_messages.append(SystemMessage(content=context_message))
            else:
                # æ²¡æœ‰æœç´¢ç»“æœçš„æƒ…å†µ
                no_result_message = "æœç´¢æ²¡æœ‰è¿”å›æœ‰æ•ˆç»“æœï¼Œè¯·åŸºäºå¸¸è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶è¯´æ˜å¯èƒ½éœ€è¦æ›´å…·ä½“çš„æœç´¢è¯ã€‚"
                final_messages.append(SystemMessage(content=no_result_message))
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            final_messages.extend(messages)
            
            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
            try:
                response = model.invoke(final_messages)
                
                # å‡†å¤‡æ¥æºä¿¡æ¯
                sources = []
                if search_results:
                    for result in search_results:
                        if not result.get("error", False):
                            sources.append({
                                "query": result["query"],
                                "content": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                                "tool": result.get("tool", "unknown")
                            })
                
                # å‡†å¤‡å“åº”å…ƒæ•°æ®
                response_metadata = {
                    "mode": "search",
                    "search_info": search_info,
                    "source_count": len(sources),
                    "sources": sources,
                    "processing_type": "è”ç½‘æœç´¢"
                }
                
                return {
                    "messages": [response],
                    "final_response": response.content,
                    "metadata": response_metadata
                }
                
            except Exception as e:
                logger.error(f"æœç´¢å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
                return {
                    "final_response": f"ç”Ÿæˆæœç´¢å›ç­”æ—¶å‡ºé”™: {str(e)}",
                    "metadata": {
                        "mode": "search",
                        "error": True,
                        "error_type": "response_generation_failed",
                        "error_message": str(e),
                        "search_info": search_info
                    }
                }
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("search_planning", search_planning_node)
        graph_builder.add_node("execute_search", execute_search_node)
        graph_builder.add_node("search_response", search_response_node)
        
        # æ·»åŠ è¾¹ï¼šè§„åˆ’ -> æœç´¢ -> å“åº”
        graph_builder.add_edge(START, "search_planning")
        graph_builder.add_edge("search_planning", "execute_search")
        graph_builder.add_edge("execute_search", "search_response")
        graph_builder.add_edge("search_response", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_deepresearch_graph(self) -> StateGraph:
        """æ„å»ºæ·±åº¦ç ”ç©¶æ¨¡å¼çš„çŠ¶æ€å›¾ - åŸºäºReActæ¨¡å¼çš„å¤šè½®æœç´¢"""
        
        def research_planning_node(state: ConversationState) -> Dict[str, Any]:
            """ç ”ç©¶è§„åˆ’èŠ‚ç‚¹ - åˆ†æé—®é¢˜å¹¶åˆ¶å®šç ”ç©¶è®¡åˆ’"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            
            # æ„å»ºç ”ç©¶è§„åˆ’æç¤º
            planning_prompt = f"""
ğŸ¤” æ·±åº¦ç ”ç©¶è§„åˆ’

ä½œä¸ºä¸“ä¸šç ”ç©¶åˆ†æå¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’ï¼š

ç ”ç©¶ä¸»é¢˜ï¼š{user_query}

è¯·åˆ†æå¹¶åˆ¶å®šç ”ç©¶è®¡åˆ’ï¼ŒåŒ…æ‹¬ï¼š

1. **ç ”ç©¶ç›®æ ‡åˆ†è§£**
   - ä¸»è¦ç ”ç©¶ç›®æ ‡
   - å…³é”®ç ”ç©¶é—®é¢˜ 
   - éœ€è¦æ”¶é›†çš„ä¿¡æ¯ç±»å‹

2. **æœç´¢ç­–ç•¥è§„åˆ’**
   - ç¬¬ä¸€è½®æœç´¢ï¼šåŸºç¡€ä¿¡æ¯å’ŒèƒŒæ™¯
   - ç¬¬äºŒè½®æœç´¢ï¼šæ·±åº¦åˆ†æå’Œä¸“ä¸šè§‚ç‚¹
   - ç¬¬ä¸‰è½®æœç´¢ï¼šæœ€æ–°å‘å±•å’Œè¶‹åŠ¿

3. **é¢„æœŸäº§å‡º**
   - æœ€ç»ˆæŠ¥å‘Šåº”åŒ…å«çš„æ ¸å¿ƒå†…å®¹
   - é‡ç‚¹å…³æ³¨çš„åˆ†æè§’åº¦

è¯·æä¾›ä¸€ä¸ªç»“æ„åŒ–çš„ç ”ç©¶è®¡åˆ’ï¼ŒæŒ‡å¯¼åç»­çš„å¤šè½®æœç´¢å’Œåˆ†æã€‚
"""
            
            planning_messages = [SystemMessage(content=planning_prompt)]
            
            try:
                response = model.invoke(planning_messages)
                research_plan = response.content.strip()
                
                # æ ¹æ®è®¡åˆ’ç”Ÿæˆç¬¬ä¸€è½®æœç´¢æŸ¥è¯¢
                query_prompt = f"""
åŸºäºä»¥ä¸‹ç ”ç©¶è®¡åˆ’ï¼Œç”Ÿæˆ3ä¸ªç¬¬ä¸€è½®æœç´¢æŸ¥è¯¢ï¼Œç”¨äºæ”¶é›†åŸºç¡€ä¿¡æ¯å’ŒèƒŒæ™¯ï¼š

ç ”ç©¶è®¡åˆ’ï¼š
{research_plan}

åŸå§‹é—®é¢˜ï¼š{user_query}

è¯·åªè¿”å›æœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼š
"""
                
                query_response = model.invoke([SystemMessage(content=query_prompt)])
                search_queries_text = query_response.content.strip()
                
                # è§£ææœç´¢æŸ¥è¯¢
                initial_queries = []
                for line in search_queries_text.split('\n'):
                    query = line.strip()
                    if query and not query.startswith('#'):
                        import re
                        query = re.sub(r'^\d+[\.ã€]\s*', '', query)
                        query = query.strip('- ')
                        if query:
                            initial_queries.append(query)
                
                initial_queries = initial_queries[:3]  # é™åˆ¶ä¸º3ä¸ªæŸ¥è¯¢
                
                if not initial_queries:
                    initial_queries = [user_query]  # å›é€€
                
                logger.info(f"ç ”ç©¶è§„åˆ’å®Œæˆï¼Œç”Ÿæˆåˆå§‹æŸ¥è¯¢: {initial_queries}")
                
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "planning",
                        "current_iteration": 1,
                        "max_iterations": 3
                    },
                    "research_plan": research_plan,
                    "research_iterations": 1,
                    "search_history": [{
                        "iteration": 1,
                        "phase": "initial_exploration",
                        "queries": initial_queries,
                        "purpose": "æ”¶é›†åŸºç¡€ä¿¡æ¯å’ŒèƒŒæ™¯"
                    }],
                    "current_findings": []
                }
                
            except Exception as e:
                logger.error(f"ç ”ç©¶è§„åˆ’å¤±è´¥: {str(e)}")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "planning",
                        "planning_error": str(e)
                    },
                    "research_plan": f"ç”±äºè§„åˆ’å¤±è´¥ï¼Œå°†ç›´æ¥æœç´¢ç”¨æˆ·é—®é¢˜: {user_query}",
                    "research_iterations": 1,
                    "search_history": [{
                        "iteration": 1,
                        "phase": "fallback",
                        "queries": [user_query],
                        "purpose": "ç›´æ¥æœç´¢ç”¨æˆ·é—®é¢˜"
                    }],
                    "current_findings": []
                }
        
        def execute_research_search_node(state: ConversationState) -> Dict[str, Any]:
            """æ‰§è¡Œç ”ç©¶æœç´¢èŠ‚ç‚¹ - æ‰§è¡Œå½“å‰è¿­ä»£çš„æœç´¢"""
            from app.llm.tools.duckduckgo_search import duckduckgo_search_tool
            
            search_history = state.get("search_history", [])
            current_iteration = state.get("research_iterations", 1)
            
            if not search_history:
                logger.error("æ²¡æœ‰æœç´¢å†å²è®°å½•")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_error": "æ²¡æœ‰æœç´¢å†å²è®°å½•"
                    }
                }
            
            # è·å–å½“å‰è¿­ä»£çš„æœç´¢ä¿¡æ¯
            current_search = None
            for search in search_history:
                if search.get("iteration") == current_iteration:
                    current_search = search
                    break
            
            if not current_search:
                logger.error(f"æ‰¾ä¸åˆ°ç¬¬ {current_iteration} æ¬¡è¿­ä»£çš„æœç´¢ä¿¡æ¯")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_error": f"æ‰¾ä¸åˆ°ç¬¬ {current_iteration} æ¬¡è¿­ä»£çš„æœç´¢ä¿¡æ¯"
                    }
                }
            
            search_queries = current_search.get("queries", [])
            search_results = []
            
            logger.info(f"å¼€å§‹ç¬¬ {current_iteration} è½®æœç´¢ï¼ŒæŸ¥è¯¢æ•°é‡: {len(search_queries)}")
            
            # æ‰§è¡Œå½“å‰è¿­ä»£çš„æ‰€æœ‰æœç´¢æŸ¥è¯¢
            for i, query in enumerate(search_queries):
                try:
                    result = duckduckgo_search_tool.invoke({"query": query})
                    if result and isinstance(result, str):
                        search_results.append({
                            "iteration": current_iteration,
                            "query": query,
                            "content": result,
                            "index": i + 1,
                            "success": True
                        })
                        logger.info(f"æŸ¥è¯¢ '{query}' æœç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(result)}")
                    else:
                        logger.warning(f"æŸ¥è¯¢ '{query}' æ²¡æœ‰è¿”å›æœ‰æ•ˆç»“æœ")
                        search_results.append({
                            "iteration": current_iteration,
                            "query": query,
                            "content": "æœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœ",
                            "index": i + 1,
                            "success": False
                        })
                except Exception as e:
                    logger.error(f"æŸ¥è¯¢ '{query}' æœç´¢å¤±è´¥: {str(e)}")
                    search_results.append({
                        "iteration": current_iteration,
                        "query": query,
                        "content": f"æœç´¢å¤±è´¥: {str(e)}",
                        "index": i + 1,
                        "success": False,
                        "error": str(e)
                    })
            
            # æ›´æ–°æœç´¢å†å²ï¼Œæ·»åŠ ç»“æœ
            updated_search_history = search_history.copy()
            for search in updated_search_history:
                if search.get("iteration") == current_iteration:
                    search["results"] = search_results
                    search["completed"] = True
                    break
            
            # æ”¶é›†å½“å‰å‘ç°
            current_findings = state.get("current_findings", [])
            new_findings = []
            for result in search_results:
                if result.get("success", False):
                    new_findings.append(f"ã€ç¬¬{current_iteration}è½®-æŸ¥è¯¢{result['index']}ã€‘{result['query']}: {result['content'][:300]}...")
            
            updated_findings = current_findings + new_findings
            
            logger.info(f"ç¬¬ {current_iteration} è½®æœç´¢å®Œæˆï¼ŒæˆåŠŸ: {len([r for r in search_results if r.get('success')])}, å¤±è´¥: {len([r for r in search_results if not r.get('success')])}")
            
            return {
                "search_history": updated_search_history,
                "current_findings": updated_findings,
                "retrieved_documents": [r["content"] for r in search_results if r.get("success", False)],
                "metadata": {
                    **state.get("metadata", {}),
                    "current_iteration": current_iteration,
                    "search_completed": True,
                    "search_results_count": len([r for r in search_results if r.get("success")])
                }
            }
        
        def research_analysis_node(state: ConversationState) -> Dict[str, Any]:
            """ç ”ç©¶åˆ†æèŠ‚ç‚¹ - åˆ†æå½“å‰ç»“æœå¹¶å†³å®šæ˜¯å¦ç»§ç»­"""
            model = self._get_model(state["model_config"])
            current_iteration = state.get("research_iterations", 1)
            max_iterations = state.get("metadata", {}).get("max_iterations", 3)
            current_findings = state.get("current_findings", [])
            research_plan = state.get("research_plan", "")
            user_query = state.get("user_query", "")
            
            # å¦‚æœå·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ ‡è®°å®Œæˆ
            if current_iteration >= max_iterations:
                logger.info(f"å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "final_report",
                        "analysis_complete": True,
                        "continue_research": False
                    }
                }
            
            # åˆ†æå½“å‰æ”¶é›†çš„ä¿¡æ¯
            findings_text = "\n\n".join(current_findings) if current_findings else "æš‚æ— æœ‰æ•ˆå‘ç°"
            
            analysis_prompt = f"""
ğŸ¤” ç ”ç©¶è¿›åº¦åˆ†æ

åŸå§‹ç ”ç©¶é—®é¢˜ï¼š{user_query}

ç ”ç©¶è®¡åˆ’ï¼š
{research_plan}

å½“å‰è¿­ä»£ï¼š{current_iteration}/{max_iterations}

å·²æ”¶é›†çš„ä¿¡æ¯ï¼š
{findings_text}

è¯·åˆ†æå½“å‰ç ”ç©¶è¿›åº¦ï¼š

1. **ä¿¡æ¯å®Œæ•´æ€§è¯„ä¼°**
   - å½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå›ç­”åŸå§‹é—®é¢˜ï¼Ÿ
   - è¿˜æœ‰å“ªäº›å…³é”®ä¿¡æ¯ç¼ºå¤±ï¼Ÿ

2. **ä¸‹ä¸€è½®æœç´¢å»ºè®®**
   - å¦‚æœéœ€è¦ç»§ç»­ç ”ç©¶ï¼Œåº”è¯¥æœç´¢ä»€ä¹ˆï¼Ÿ
   - å»ºè®®3ä¸ªå…·ä½“çš„æœç´¢æŸ¥è¯¢

3. **ç ”ç©¶å†³ç­–**
   - æ˜¯å¦åº”è¯¥ç»§ç»­ä¸‹ä¸€è½®æœç´¢ï¼Ÿ
   - è¿˜æ˜¯å¯ä»¥å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Ÿ

è¯·æœ€åæ˜ç¡®å›ç­”ï¼šCONTINUEï¼ˆç»§ç»­ç ”ç©¶ï¼‰æˆ– COMPLETEï¼ˆå®Œæˆç ”ç©¶ï¼‰
"""
            
            try:
                response = model.invoke([SystemMessage(content=analysis_prompt)])
                analysis_result = response.content.strip()
                
                # åˆ¤æ–­æ˜¯å¦ç»§ç»­ç ”ç©¶
                continue_research = "CONTINUE" in analysis_result.upper() and "COMPLETE" not in analysis_result.upper()
                
                if continue_research and current_iteration < max_iterations:
                    # æå–ä¸‹ä¸€è½®æœç´¢æŸ¥è¯¢
                    next_queries = []
                    lines = analysis_result.split('\n')
                    capture_queries = False
                    for line in lines:
                        line = line.strip()
                        if 'æœç´¢æŸ¥è¯¢' in line or 'queries' in line.lower():
                            capture_queries = True
                            continue
                        if capture_queries and line:
                            if line.startswith(('1.', '2.', '3.', '-', 'â€¢')):
                                import re
                                query = re.sub(r'^[\d\.\-\â€¢\s]+', '', line).strip()
                                if query:
                                    next_queries.append(query)
                    
                    # å¦‚æœæ²¡æœ‰æå–åˆ°æŸ¥è¯¢ï¼Œç”Ÿæˆé»˜è®¤æŸ¥è¯¢
                    if not next_queries:
                        next_queries = [f"{user_query} æœ€æ–°å‘å±•", f"{user_query} ä¸“å®¶è§‚ç‚¹", f"{user_query} æ¡ˆä¾‹åˆ†æ"]
                    
                    next_queries = next_queries[:3]  # é™åˆ¶ä¸º3ä¸ª
                    
                    # æ›´æ–°æœç´¢å†å²ï¼Œæ·»åŠ ä¸‹ä¸€è½®
                    search_history = state.get("search_history", [])
                    next_iteration = current_iteration + 1
                    search_history.append({
                        "iteration": next_iteration,
                        "phase": f"deep_dive_{next_iteration}",
                        "queries": next_queries,
                        "purpose": f"ç¬¬{next_iteration}è½®æ·±åº¦ç ”ç©¶"
                    })
                    
                    logger.info(f"å†³å®šç»§ç»­ç¬¬ {next_iteration} è½®ç ”ç©¶ï¼ŒæŸ¥è¯¢: {next_queries}")
                    
                    return {
                        "search_history": search_history,
                        "research_iterations": next_iteration,
                        "metadata": {
                            **state.get("metadata", {}),
                            "research_phase": f"iteration_{next_iteration}",
                            "continue_research": True,
                            "analysis_result": analysis_result[:500] + "..."  # æˆªæ–­ä»¥èŠ‚çœç©ºé—´
                        }
                    }
                else:
                    logger.info("åˆ†æå†³å®šå®Œæˆç ”ç©¶ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
                    return {
                        "metadata": {
                            **state.get("metadata", {}),
                            "research_phase": "final_report",
                            "continue_research": False,
                            "analysis_result": analysis_result[:500] + "..."
                        }
                    }
                    
            except Exception as e:
                logger.error(f"ç ”ç©¶åˆ†æå¤±è´¥: {str(e)}")
                # åˆ†æå¤±è´¥ï¼Œé»˜è®¤å®Œæˆç ”ç©¶
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "final_report",
                        "continue_research": False,
                        "analysis_error": str(e)
                    }
                }
        
        def generate_research_report_node(state: ConversationState) -> Dict[str, Any]:
            """ç”Ÿæˆç ”ç©¶æŠ¥å‘ŠèŠ‚ç‚¹ - åŸºäºæ‰€æœ‰æ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
            model = self._get_model(state["model_config"])
            user_query = state.get("user_query", "")
            research_plan = state.get("research_plan", "")
            current_findings = state.get("current_findings", [])
            search_history = state.get("search_history", [])
            messages = state.get("messages") or []
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_deepresearch_prompt()
            
            # å‡†å¤‡ç ”ç©¶è¿‡ç¨‹æ€»ç»“
            research_summary = []
            for search in search_history:
                iteration = search.get("iteration", 0)
                phase = search.get("phase", "unknown")
                queries = search.get("queries", [])
                purpose = search.get("purpose", "")
                research_summary.append(f"ç¬¬{iteration}è½® ({phase}): {purpose} - æŸ¥è¯¢: {', '.join(queries)}")
            
            research_process = "\n".join(research_summary)
            
            # æ•´ç†æ‰€æœ‰å‘ç°
            all_findings = "\n\n".join(current_findings) if current_findings else "æœªæ”¶é›†åˆ°æœ‰æ•ˆä¿¡æ¯"
            
            # æ„å»ºæœ€ç»ˆæŠ¥å‘Šç”Ÿæˆæç¤º
            report_prompt = f"""
åŸºäºæ·±åº¦ç ”ç©¶ç»“æœï¼Œè¯·ç”Ÿæˆä¸€ä»½å…¨é¢çš„ç ”ç©¶æŠ¥å‘Šï¼š

## ç ”ç©¶èƒŒæ™¯
åŸå§‹é—®é¢˜ï¼š{user_query}

ç ”ç©¶è®¡åˆ’ï¼š
{research_plan}

## ç ”ç©¶è¿‡ç¨‹
{research_process}

## æ”¶é›†çš„ä¿¡æ¯
{all_findings}

## è¦æ±‚
è¯·ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š

1. **æ‰§è¡Œæ‘˜è¦** - æ ¸å¿ƒå‘ç°å’Œå…³é”®ç»“è®º
2. **è¯¦ç»†åˆ†æ** - åˆ†ä¸»é¢˜çš„æ·±å…¥åˆ†æ
3. **å…³é”®å‘ç°** - é‡è¦æ•°æ®å’Œæ´å¯Ÿ
4. **å¤šå…ƒè§†è§’** - ä¸åŒè§’åº¦çš„è§‚ç‚¹
5. **ç»“è®ºä¸å»ºè®®** - æ€»ç»“å’Œå»ºè®®
6. **ç ”ç©¶å±€é™** - æ‰¿è®¤ä¿¡æ¯çš„é™åˆ¶

è¯·ç¡®ä¿æŠ¥å‘Šï¼š
- ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸¥å¯†
- åŸºäºå®é™…æ”¶é›†çš„ä¿¡æ¯
- æä¾›æœ‰æ´å¯ŸåŠ›çš„åˆ†æ
- ä½¿ç”¨ä¸­æ–‡æ’°å†™
"""
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            final_messages = [SystemMessage(content=system_prompt)]
            final_messages.append(SystemMessage(content=report_prompt))
            final_messages.extend(messages)
            
            try:
                response = model.invoke(final_messages)
                
                # å‡†å¤‡å“åº”å…ƒæ•°æ®
                response_metadata = {
                    "mode": "deepresearch",
                    "total_iterations": len(search_history),
                    "total_findings": len(current_findings),
                    "research_process": research_process,
                    "processing_type": "æ·±åº¦ç ”ç©¶æŠ¥å‘Š"
                }
                
                # æ·»åŠ æœç´¢æ¥æºä¿¡æ¯
                sources = []
                for finding in current_findings:
                    if "ã€‘" in finding:
                        source_info = finding.split("ã€‘")[0] + "ã€‘"
                        content_preview = finding.split("ã€‘")[1][:200] if "ã€‘" in finding else finding[:200]
                        sources.append({
                            "source": source_info,
                            "content": content_preview + "..." if len(content_preview) == 200 else content_preview
                        })
                
                response_metadata["sources"] = sources[:10]  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªæ¥æº
                
                logger.info(f"æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼Œæ€»è¿­ä»£æ¬¡æ•°: {len(search_history)}, å‘ç°æ•°é‡: {len(current_findings)}")
                
                return {
                    "messages": [response],
                    "final_response": response.content,
                    "metadata": response_metadata
                }
                
            except Exception as e:
                logger.error(f"ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
                return {
                    "final_response": f"ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}",
                    "metadata": {
                        "mode": "deepresearch",
                        "error": True,
                        "error_type": "report_generation_failed",
                        "error_message": str(e),
                        "total_iterations": len(search_history),
                        "total_findings": len(current_findings)
                    }
                }
        
        def route_research_flow(state: ConversationState) -> str:
            """è·¯ç”±ç ”ç©¶æµç¨‹ - å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ"""
            metadata = state.get("metadata", {})
            research_phase = metadata.get("research_phase", "planning")
            continue_research = metadata.get("continue_research", True)
            
            if research_phase == "planning":
                return "execute_search"
            elif research_phase.startswith("iteration_") or metadata.get("search_completed", False):
                if continue_research:
                    return "execute_search"
                else:
                    return "generate_report"
            elif research_phase == "final_report":
                return "generate_report"
            else:
                # é»˜è®¤åˆ†æå½“å‰çŠ¶æ€
                return "analyze_progress"
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("research_planning", research_planning_node)
        graph_builder.add_node("execute_search", execute_research_search_node)
        graph_builder.add_node("analyze_progress", research_analysis_node)
        graph_builder.add_node("generate_report", generate_research_report_node)
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
        graph_builder.add_edge(START, "research_planning")
        
        # ä»è§„åˆ’åˆ°æ‰§è¡Œæœç´¢
        graph_builder.add_edge("research_planning", "execute_search")
        
        # ä»æœç´¢æ‰§è¡Œåˆ°åˆ†æè¿›åº¦
        graph_builder.add_edge("execute_search", "analyze_progress")
        
        # ä»åˆ†æè¿›åº¦çš„æ¡ä»¶è·¯ç”±
        graph_builder.add_conditional_edges(
            "analyze_progress",
            route_research_flow,
            {
                "execute_search": "execute_search",
                "generate_report": "generate_report"
            }
        )
        
        # ç”ŸæˆæŠ¥å‘Šåˆ°ç»“æŸ
        graph_builder.add_edge("generate_report", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    async def _get_graph(self, mode: str, conversation_id: Optional[UUID] = None):
        """è·å–å¯¹åº”æ¨¡å¼çš„å›¾ï¼ˆæ¯æ¬¡ä¸ºç‰¹å®šconversation_idåˆ›å»ºç‹¬ç«‹å®ä¾‹ï¼‰"""
        # æ„å»ºåŸºç¡€å›¾ï¼ˆæœªç¼–è¯‘ï¼‰
        if mode == "chat":
            graph_builder = self._build_chat_graph()
        elif mode == "rag":
            graph_builder = self._build_rag_graph()
        elif mode == "agent":
            graph_builder = self._build_agent_graph()
        elif mode == "search":
            graph_builder = self._build_search_graph()
        elif mode == "deepresearch":
            graph_builder = self._build_deepresearch_graph()
        else:
            # é»˜è®¤ä½¿ç”¨èŠå¤©æ¨¡å¼
            graph_builder = self._build_chat_graph()
        
        # æ¯æ¬¡éƒ½é‡æ–°ç¼–è¯‘å›¾ä»¥ç¡®ä¿checkpointeræ­£ç¡®ç»‘å®š
        try:
            checkpointer = await get_checkpointer(conversation_id)
            # ç¼–è¯‘å›¾å¹¶æ·»åŠ checkpointer
            compiled_graph = graph_builder.compile(checkpointer=checkpointer)
            logger.debug(f"ä¸ºå¯¹è¯ {conversation_id} åˆ›å»ºäº†å¸¦checkpointerçš„å›¾")
            return compiled_graph
        except Exception as e:
            # å¦‚æœcheckpointerå¤±è´¥ï¼Œç¼–è¯‘ä¸å¸¦checkpointerçš„å›¾
            logger.warning(f"Warning: æ— æ³•åˆ›å»ºcheckpointerï¼Œä½¿ç”¨åŸºç¡€å›¾: {e}")
            return graph_builder.compile()
    
    async def process_conversation(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        mode: str = "chat",
        system_prompt: Optional[str] = None,
        retrieved_documents: Optional[List[str]] = None,
        available_tools: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        conversation_id: Optional[UUID] = None,
    ) -> AsyncGenerator[str, None]:
        """
        ä½¿ç”¨ LangGraph å¤„ç†å¯¹è¯ï¼Œæ”¯æŒ checkpointer çŠ¶æ€æŒä¹…åŒ–
        
        Args:
            messages: æ¶ˆæ¯å†å²
            model_config: æ¨¡å‹é…ç½®
            mode: å¤„ç†æ¨¡å¼ (chat/rag/agent)
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨å¯¹åº”æ¨¡å¼çš„é»˜è®¤æç¤ºè¯ï¼‰
            retrieved_documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆRAGæ¨¡å¼ï¼‰
            available_tools: å¯ç”¨å·¥å…·ï¼ˆAgentæ¨¡å¼ï¼‰
            metadata: é¢å¤–å…ƒæ•°æ®
            conversation_id: å¯¹è¯IDï¼ˆç”¨äºcheckpointerï¼‰
        """
        try:
            # è·å–å¯¹åº”æ¨¡å¼çš„å›¾
            graph = await self._get_graph(mode, conversation_id)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            langchain_messages = convert_messages_to_langchain(messages)
            
            # æ„å»ºåˆå§‹çŠ¶æ€
            initial_state: ConversationState = {
                "messages": langchain_messages,
                "model_config": model_config,
                "system_prompt": system_prompt,
                "mode": mode,
                "retrieved_documents": retrieved_documents,
                "available_tools": available_tools,
                "metadata": metadata or {},
                "user_query": messages[-1]["content"] if messages else None,
                "final_response": None,
                "conversation_id": str(conversation_id) if conversation_id else None,
            }
            
            # å‡†å¤‡å›¾æ‰§è¡Œé…ç½®
            config = {}
            if conversation_id:
                config = get_conversation_config(conversation_id)
            
            # æ‰§è¡Œå›¾
            try:
                result = await graph.ainvoke(initial_state, config=config)
                
                # æå–æœ€ç»ˆå“åº”
                final_response = result.get("final_response", "")
                result_metadata = result.get("metadata", {})
                
                if final_response:
                    # ç”Ÿæˆæµå¼å“åº”
                    chunk_dict = {
                        "content": final_response,
                        "done": True,
                        "error": False,
                        "message": None,
                        "metadata": result_metadata
                    }
                    
                    # æ·»åŠ RAGæ¨¡å¼çš„æ¥æºä¿¡æ¯
                    if mode == "rag" and result_metadata.get("sources"):
                        chunk_dict["sources"] = result_metadata["sources"]
                    
                    # æ·»åŠ Agentæ¨¡å¼çš„å·¥å…·ä½¿ç”¨ä¿¡æ¯
                    if mode == "agent":
                        chunk_dict["is_tool_use"] = result_metadata.get("is_tool_use", False)
                    
                    yield json.dumps(chunk_dict)
                else:
                    # å¦‚æœæ²¡æœ‰æœ€ç»ˆå“åº”ï¼Œè¿”å›é”™è¯¯
                    yield json.dumps({
                        "content": "ç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯",
                        "done": True,
                        "error": True,
                        "message": "No response generated",
                        "metadata": {}
                    })
                    
            except Exception as e:
                # å›¾æ‰§è¡Œé”™è¯¯
                yield json.dumps({
                    "content": f"å¤„ç†å¯¹è¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                    "done": True,
                    "error": True,
                    "message": str(e),
                    "metadata": {}
                })
                
        except Exception as e:
            # æ•´ä½“é”™è¯¯å¤„ç†
            yield json.dumps({
                "content": f"åˆå§‹åŒ–å¯¹è¯å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "done": True,
                "error": True,
                "message": str(e),
                "metadata": {}
            })
    
    # ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•
    async def process_chat(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„èŠå¤©å¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="chat",
            system_prompt=system_prompt
        ):
            yield chunk
    
    async def process_rag(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        retrieved_documents: List[str],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„RAGå¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="rag",
            system_prompt=system_prompt,
            retrieved_documents=retrieved_documents
        ):
            yield chunk
    
    async def process_agent(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        available_tools: List[str],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„Agentå¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="agent",
            system_prompt=system_prompt,
            available_tools=available_tools
        ):
            yield chunk
    
    async def process_search(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„æœç´¢å¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="search",
            system_prompt=system_prompt
        ):
            yield chunk
    
    async def process_deepresearch(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„æ·±åº¦ç ”ç©¶å¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="deepresearch",
            system_prompt=system_prompt
        ):
            yield chunk
    
    def clear_model_cache(self):
        """æ¸…é™¤æ¨¡å‹ç¼“å­˜"""
        self._model_cache.clear()
        self._graphs.clear()
    
    async def clear_conversation_state(self, conversation_id: UUID):
        """æ¸…é™¤ç‰¹å®šå¯¹è¯çš„çŠ¶æ€"""
        from app.llm.core.checkpointer import clear_conversation_checkpoint
        await clear_conversation_checkpoint(conversation_id)
    
    def get_cached_models(self) -> List[str]:
        """è·å–å·²ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self._model_cache.keys())
    
    def get_cached_graphs(self) -> List[str]:
        """è·å–å·²ç¼“å­˜çš„å›¾åˆ—è¡¨"""
        return list(self._graphs.keys())

    async def estimate_tokens(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> Dict[str, int]:
        """
        ä¼°ç®—tokenä½¿ç”¨é‡
        
        Args:
            messages: æ¶ˆæ¯å†å²
            model_config: æ¨¡å‹é…ç½®
            system_prompt: ç³»ç»Ÿæç¤º
            
        Returns:
            tokenä½¿ç”¨æƒ…å†µ
        """
        # è·å–æ¨¡å‹å®ä¾‹
        model = self._get_model(model_config)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        final_messages = []
        if system_prompt:
            final_messages.append(SystemMessage(content=system_prompt))
        
        # è½¬æ¢å¹¶æ·»åŠ å†å²æ¶ˆæ¯
        langchain_messages = convert_messages_to_langchain(messages)
        final_messages.extend(langchain_messages)
        
        # ä½¿ç”¨LangChainçš„tokenè®¡ç®—åŠŸèƒ½
        try:
            # å¤§å¤šæ•°LangChainæ¨¡å‹éƒ½æœ‰get_num_tokensæ–¹æ³•
            if hasattr(model, 'get_num_tokens_from_messages'):
                token_count = model.get_num_tokens_from_messages(final_messages)
            elif hasattr(model, 'get_num_tokens'):
                # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„æ¶ˆæ¯tokenè®¡ç®—æ–¹æ³•ï¼Œå°è¯•è½¬æ¢ä¸ºæ–‡æœ¬
                text = "\n".join([msg.content for msg in final_messages])
                token_count = model.get_num_tokens(text)
            else:
                # ç®€å•ä¼°ç®—ï¼šæ¯4ä¸ªå­—ç¬¦çº¦1ä¸ªtoken
                text = "\n".join([msg.content for msg in final_messages])
                token_count = len(text) // 4
        except Exception:
            # å›é€€åˆ°ç®€å•ä¼°ç®—
            text = "\n".join([msg.content for msg in final_messages])
            token_count = len(text) // 4
        
        return {
            "prompt_tokens": token_count,
            "max_tokens": model_config.get("max_tokens", 4000),
            "available_tokens": max(0, model_config.get("max_tokens", 4000) - token_count),
        } 
    
    
    async def process_file_to_documents(
        self, 
        file_path: str, 
        file_type: str,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        user_id: Optional[str] = None,
        file_id: Optional[str] = None,
        file_name: Optional[str] = None
    ) -> Tuple[List[Document], Dict]:
        """
        å¤„ç†æ–‡ä»¶å†…å®¹å¹¶è¿”å›åˆ†å‰²åçš„Documentå¯¹è±¡åˆ—è¡¨
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å 
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰
            file_id: æ–‡ä»¶IDï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰
            file_name: åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰
            
        Returns:
            (åˆ†å‰²åçš„Documentå¯¹è±¡åˆ—è¡¨, æ–‡ä»¶å…ƒæ•°æ®)
        """
        try:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            if not self.file_mgr.validate_file_type(file_type):
                raise FileProcessingException(detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
            
            # åŠ è½½æ–‡æ¡£
            documents = await self.file_mgr.load_documents(file_path, file_type)
            
            if not documents:
                raise FileProcessingException(detail="æ— æ³•ä»æ–‡ä»¶ä¸­æå–å†…å®¹")
            
            # æ”¶é›†åŸå§‹æ–‡æ¡£å…ƒæ•°æ®
            metadata = self.file_mgr.collect_metadata(documents, file_type)
            
            # åˆ†å‰²æ–‡æ¡£
            split_documents = self.file_mgr.split_documents(documents, chunk_size, chunk_overlap)
            
            # ä¸ºæ¯ä¸ªåˆ†å‰²çš„æ–‡æ¡£æ·»åŠ éš”ç¦»å’Œè¿½è¸ªå…ƒæ•°æ®
            enhanced_documents = []
            for i, doc in enumerate(split_documents):
                enhanced_metadata = {
                    **doc.metadata,
                    "chunk_index": i,
                    "total_chunks": len(split_documents),
                }
                
                # æ·»åŠ éš”ç¦»ç›¸å…³çš„å…ƒæ•°æ®
                if user_id:
                    enhanced_metadata["user_id"] = user_id
                if file_id:
                    enhanced_metadata["file_id"] = file_id
                if file_name:
                    enhanced_metadata["original_filename"] = file_name
                    enhanced_metadata["source"] = file_name
                
                # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
                import datetime
                enhanced_metadata["processed_at"] = datetime.datetime.now().isoformat()
                enhanced_metadata["file_type"] = file_type
                
                # è¿‡æ»¤å¤æ‚çš„å…ƒæ•°æ®ç±»å‹ï¼Œåªä¿ç•™å‘é‡æ•°æ®åº“æ”¯æŒçš„åŸºç¡€ç±»å‹
                filtered_metadata = {
                    k: v for k, v in enhanced_metadata.items() 
                    if isinstance(v, (str, bool, int, float)) and v is not None
                }
                
                enhanced_doc = Document(
                    page_content=doc.page_content,
                    metadata=filtered_metadata
                )
                enhanced_documents.append(enhanced_doc)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_content = "\n\n".join([doc.page_content for doc in documents])
            metadata.update({
                "original_document_count": len(documents),
                "split_document_count": len(split_documents),
                "total_character_count": len(total_content),
                "chunk_size": chunk_size or getattr(self, 'default_chunk_size', 1000),
                "chunk_overlap": chunk_overlap or getattr(self, 'default_chunk_overlap', 200),
                "user_id": user_id,
                "file_id": file_id,
                "file_name": file_name,
                "file_type": file_type
            })
            
            logger.info(f"æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_name or file_path}, åˆ†å‰²ä¸º {len(enhanced_documents)} ä¸ªæ–‡æ¡£å—")
            
            return enhanced_documents, metadata
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
            raise FileProcessingException(detail=f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
    