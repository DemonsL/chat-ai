import json
from typing import AsyncGenerator, Dict, List, Optional, Any, Annotated, Literal
from uuid import UUID
from enum import Enum
from loguru import logger

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, SystemMessage
from langchain.chat_models import init_chat_model
from typing_extensions import TypedDict
from pydantic import BaseModel, Field

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

from app.llm.core.base import (
    StreamingChunk,
    create_chat_model,
    convert_messages_to_langchain,
    stream_chat_model_response
)
from app.llm.core.prompts import prompt_manager
from app.llm.core.checkpointer import get_checkpointer, get_conversation_config


class ConversationMode(str, Enum):
    """ä¼šè¯æ¨¡å¼æšä¸¾"""
    CHAT = "chat"
    RAG = "rag"
    AGENT = "agent"
    SEARCH = "search"
    DEEPRESEARCH = "deepresearch"


class QuestionAnalysisResult(BaseModel):
    """é—®é¢˜åˆ†æç»“æœçš„ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹"""
    question_type: Literal["knowledge_retrieval", "document_processing", "general_chat"] = Field(
        description="é—®é¢˜ç±»å‹ï¼šknowledge_retrieval(çŸ¥è¯†æ£€ç´¢)ã€document_processing(æ–‡æ¡£å¤„ç†)ã€general_chat(ä¸€èˆ¬å¯¹è¯)"
    )
    processing_strategy: Literal["standard_rag", "summarization", "analysis", "translation", "direct_answer"] = Field(
        description="å¤„ç†ç­–ç•¥ï¼šstandard_rag(æ ‡å‡†RAG)ã€summarization(æ€»ç»“)ã€analysis(åˆ†æ)ã€translation(ç¿»è¯‘)ã€direct_answer(ç›´æ¥å›ç­”)"
    )
    needs_retrieval: bool = Field(
        description="æ˜¯å¦éœ€è¦æ£€ç´¢æ–‡æ¡£å†…å®¹"
    )
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="åˆ†æç»“æœçš„ç½®ä¿¡åº¦ï¼ŒèŒƒå›´0.0-1.0"
    )
    reasoning: str = Field(
        description="åˆ†ææ¨ç†è¿‡ç¨‹å’Œä¾æ®"
    )


class ConversationState(TypedDict):
    """LangGraph å¯¹è¯çŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[BaseMessage], add_messages]
    model_config: Dict[str, Any]
    system_prompt: Optional[str]
    mode: str
    retrieved_documents: Optional[List[str]]
    available_tools: Optional[List[str]]
    metadata: Optional[Dict[str, Any]]
    user_query: Optional[str]
    final_response: Optional[str]
    conversation_id: Optional[str]  # æ·»åŠ å¯¹è¯IDç”¨äºcheckpointer
    # æ·±åº¦ç ”ç©¶ç›¸å…³çŠ¶æ€
    research_iterations: Optional[int]
    search_history: Optional[List[Dict[str, Any]]]
    current_findings: Optional[List[str]]
    research_plan: Optional[str]


class LLMManager:
    """
    åŸºäº LangGraph çš„ LLMç¼–æ’æœåŠ¡
    ä½¿ç”¨çŠ¶æ€å›¾ç®¡ç†å¤šè½®å¯¹è¯æµç¨‹ï¼Œæ”¯æŒ PostgresSaver checkpointer
    """
    
    def __init__(self, retrieval_service=None):
        self._model_cache = {}  # ç¼“å­˜å·²åˆ›å»ºçš„æ¨¡å‹å®ä¾‹
        self._graphs = {}  # ç¼“å­˜ä¸åŒæ¨¡å¼çš„å›¾
        self.retrieval_service = retrieval_service  # æ£€ç´¢æœåŠ¡ä¾èµ–
        
    def _get_model(self, model_config: Dict[str, Any]) -> BaseChatModel:
        """è·å–æˆ–åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = f"{model_config['provider']}-{model_config['model_id']}"
        
        if cache_key not in self._model_cache:
            try:
                # å‡†å¤‡æ¨¡å‹å‚æ•°
                model_params = {
                    "temperature": model_config.get("temperature", 0.7),
                    "max_tokens": model_config.get("max_tokens"),
                }
                
                # æ·»åŠ é¢å¤–å‚æ•°
                if model_config.get("extra_params"):
                    model_params.update(model_config["extra_params"])
                
                # è¿‡æ»¤Noneå€¼
                model_params = {k: v for k, v in model_params.items() if v is not None}
                
                # åˆ›å»ºæ¨¡å‹
                self._model_cache[cache_key] = create_chat_model(
                    provider=model_config["provider"],
                    model=model_config["model_id"],
                    **model_params
                )
            except Exception as e:
                raise ValueError(f"åˆ›å»ºæ¨¡å‹å¤±è´¥ {model_config['provider']}/{model_config['model_id']}: {str(e)}")
        
        return self._model_cache[cache_key]
    
    def _build_chat_graph(self) -> StateGraph:
        """æ„å»ºèŠå¤©æ¨¡å¼çš„çŠ¶æ€å›¾"""
        def chat_node(state: ConversationState) -> Dict[str, Any]:
            """èŠå¤©èŠ‚ç‚¹å¤„ç†å‡½æ•°"""
            model = self._get_model(state["model_config"])
            
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            final_messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤º - ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_chat_prompt()
            final_messages.append(SystemMessage(content=system_prompt))
            
            # å®‰å…¨åœ°æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_messages = state.get("messages") or []
            final_messages.extend(user_messages)
            
            # è°ƒç”¨æ¨¡å‹
            response = model.invoke(final_messages)
            
            return {
                "messages": [response],
                "final_response": response.content,
                "metadata": {"mode": "chat"}
            }
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        graph_builder.add_node("chat", chat_node)
        graph_builder.add_edge(START, "chat")
        graph_builder.add_edge("chat", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_rag_graph(self) -> StateGraph:
        """æ„å»ºRAGæ¨¡å¼çš„çŠ¶æ€å›¾"""
        
        def analyze_question_node(state: ConversationState) -> Dict[str, Any]:
            """é—®é¢˜åˆ†æèŠ‚ç‚¹ - ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½é—®é¢˜åˆ†æ"""
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            model = self._get_model(state["model_config"])
            
            # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®é¢˜åˆ†ææç¤ºè¯
            analysis_prompt = prompt_manager.get_question_analysis_prompt(user_query=user_query)

            try:
                # é¦–å…ˆå°è¯•ä½¿ç”¨with_structured_outputè¿›è¡Œç»“æ„åŒ–è¾“å‡º
                try:
                    structured_model = model.with_structured_output(QuestionAnalysisResult)
                    analysis_messages = [SystemMessage(content=analysis_prompt)]
                    analysis_result: QuestionAnalysisResult = structured_model.invoke(analysis_messages)
                    
                    logger.info(f"LLMç»“æ„åŒ–åˆ†ææˆåŠŸ: {analysis_result.question_type} -> {analysis_result.processing_strategy} (éœ€è¦æ£€ç´¢: {analysis_result.needs_retrieval}, ç½®ä¿¡åº¦: {analysis_result.confidence})")
                    logger.info(f"åˆ†ææ¨ç†: {analysis_result.reasoning}")
                    
                    return {
                        "metadata": {
                            **state.get("metadata", {}),
                            "question_type": analysis_result.question_type,
                            "processing_strategy": analysis_result.processing_strategy,
                            "needs_retrieval": analysis_result.needs_retrieval,
                            "analysis_confidence": analysis_result.confidence,
                            "analysis_reasoning": analysis_result.reasoning,
                            "analysis_method": "llm_structured",
                            "analysis_completed": True
                        }
                    }
                    
                except Exception as structured_error:
                    # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°JSONè§£ææ–¹å¼
                    logger.warning(f"ç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°JSONè§£æ: {str(structured_error)}")
                    
                    # è°ƒç”¨LLMè¿›è¡Œé—®é¢˜åˆ†æï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
                    analysis_messages = [SystemMessage(content=analysis_prompt)]
                    response = model.invoke(analysis_messages)
                    analysis_text = response.content.strip()
                    
                    # è§£æLLMè¿”å›çš„JSONç»“æœ
                    import json
                    import re
                    
                    # å°è¯•æå–JSONéƒ¨åˆ†
                    json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        analysis_data = json.loads(json_str)
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•ç›´æ¥è§£æ
                        analysis_data = json.loads(analysis_text)
                    
                    # ä½¿ç”¨Pydanticæ¨¡å‹éªŒè¯æ•°æ®
                    analysis_result = QuestionAnalysisResult(**analysis_data)
                    
                    logger.info(f"LLM JSONè§£ææˆåŠŸ: {analysis_result.question_type} -> {analysis_result.processing_strategy} (éœ€è¦æ£€ç´¢: {analysis_result.needs_retrieval}, ç½®ä¿¡åº¦: {analysis_result.confidence})")
                    logger.info(f"åˆ†ææ¨ç†: {analysis_result.reasoning}")
                    
                    return {
                        "metadata": {
                            **state.get("metadata", {}),
                            "question_type": analysis_result.question_type,
                            "processing_strategy": analysis_result.processing_strategy,
                            "needs_retrieval": analysis_result.needs_retrieval,
                            "analysis_confidence": analysis_result.confidence,
                            "analysis_reasoning": analysis_result.reasoning,
                            "analysis_method": "llm_json_fallback",
                            "analysis_completed": True
                        }
                    }
                
            except Exception as e:
                logger.error(f"LLMé—®é¢˜åˆ†æå®Œå…¨å¤±è´¥: {str(e)}")
                
                # åˆ†æå¤±è´¥ï¼Œè¿”å›ç½‘ç»œç¹å¿™çŠ¶æ€
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "analysis_error": True,
                        "error_message": "ç½‘ç»œç¹å¿™ï¼Œè¯·ç¨åé‡è¯•",
                        "analysis_completed": False
                    }
                }
        
        def retrieve_documents_node(state: ConversationState) -> Dict[str, Any]:
            """æ–‡æ¡£æ£€ç´¢èŠ‚ç‚¹ - æ‰§è¡ŒçœŸæ­£çš„æ–‡æ¡£æ£€ç´¢"""
            import asyncio
            
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            metadata = state.get("metadata", {})
            
            # è·å–æ–‡ä»¶IDåˆ—è¡¨å’Œç”¨æˆ·ID
            available_file_ids = metadata.get("available_file_ids") or []
            user_id_str = metadata.get("user_id")
            
            retrieved_docs = []
            retrieval_info = {}
            
            if available_file_ids and user_id_str and self.retrieval_service:
                try:
                    from uuid import UUID
                    
                    logger.info(f"å¼€å§‹æ£€ç´¢ {len(available_file_ids)} ä¸ªæ–‡ä»¶çš„ç›¸å…³å†…å®¹ï¼ŒæŸ¥è¯¢: '{user_query}'")
                    
                    # è½¬æ¢å­—ç¬¦ä¸²IDä¸ºUUID
                    file_ids = [UUID(fid) for fid in available_file_ids]
                    user_id = UUID(user_id_str)
                    
                    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œå¼‚æ­¥æ£€ç´¢æ“ä½œ
                    async def _async_retrieve():
                        return await self.retrieval_service.retrieve_documents(
                            query=user_query,
                            file_ids=file_ids,
                            user_id=user_id,
                            top_k=5,
                            similarity_threshold=0.3  # ä½¿ç”¨è¾ƒä½çš„é˜ˆå€¼ä»¥è·å¾—æ›´å¤šç»“æœ
                        )
                    
                    # åœ¨æ–°äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æ“ä½œ
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        retrieved_docs = loop.run_until_complete(_async_retrieve())
                    finally:
                        loop.close()
                    
                    retrieval_info = {
                        "file_count": len(available_file_ids),
                        "query": user_query,
                        "document_count": len(retrieved_docs),
                        "status": "æ£€ç´¢æˆåŠŸ",
                        "user_id": user_id_str
                    }
                    
                    logger.info(f"æ–‡æ¡£æ£€ç´¢æˆåŠŸ: æ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                    
                except Exception as e:
                    logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
                    retrieval_info = {
                        "error": str(e),
                        "status": "æ£€ç´¢å¤±è´¥",
                        "file_count": len(available_file_ids),
                        "query": user_query,
                        "user_id": user_id_str
                    }
            else:
                missing_items = []
                if not available_file_ids:
                    missing_items.append("file_ids")
                if not user_id_str:
                    missing_items.append("user_id")
                if not self.retrieval_service:
                    missing_items.append("retrieval_service")
                
                retrieval_info = {
                    "status": f"æ£€ç´¢æ¡ä»¶ä¸æ»¡è¶³ï¼Œç¼ºå°‘: {', '.join(missing_items)}",
                    "file_count": len(available_file_ids),
                    "has_service": bool(self.retrieval_service),
                    "has_user_id": bool(user_id_str),
                    "query": user_query
                }
                logger.warning(f"æ£€ç´¢èŠ‚ç‚¹ï¼šæ£€ç´¢æ¡ä»¶ä¸æ»¡è¶³ï¼Œç¼ºå°‘: {', '.join(missing_items)}")
            
            return {
                "retrieved_documents": retrieved_docs,
                "metadata": {
                    **state.get("metadata", {}),
                    "retrieval_completed": True,
                    "retrieval_info": retrieval_info,
                    "document_count": len(retrieved_docs)
                }
            }
        
        def rag_response_node(state: ConversationState) -> Dict[str, Any]:
            """RAGå“åº”èŠ‚ç‚¹ - æ ¹æ®æ£€ç´¢ç»“æœå’Œç”¨æˆ·é—®é¢˜ç”Ÿæˆå›ç­”"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            retrieved_docs = state.get("retrieved_documents") or []
            metadata = state.get("metadata", {})
            
            processing_strategy = metadata.get("processing_strategy", "standard_rag")
            needs_retrieval = metadata.get("needs_retrieval", True)
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_rag_prompt()
            
            # æ ¹æ®å¤„ç†ç­–ç•¥è°ƒæ•´ç³»ç»Ÿæç¤º
            if processing_strategy == "direct_answer":
                system_prompt += "\n\n## å½“å‰ä»»åŠ¡\nè¯·åŸºäºå¸¸è¯†ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸éœ€è¦æ£€ç´¢æ–‡æ¡£ã€‚"
            elif retrieved_docs:
                if processing_strategy == "summarization":
                    system_prompt += "\n\n## å½“å‰ä»»åŠ¡\nè¯·å¯¹æä¾›çš„æ–‡æ¡£å†…å®¹è¿›è¡Œæ€»ç»“ã€‚"
                elif processing_strategy == "analysis":
                    system_prompt += "\n\n## å½“å‰ä»»åŠ¡\nè¯·å¯¹æä¾›çš„æ–‡æ¡£å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æã€‚"
                elif processing_strategy == "translation":
                    system_prompt += "\n\n## å½“å‰ä»»åŠ¡\nè¯·å¯¹æä¾›çš„æ–‡æ¡£å†…å®¹è¿›è¡Œç¿»è¯‘ã€‚"
                else:
                    system_prompt += "\n\n## å½“å‰ä»»åŠ¡\nè¯·åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            else:
                system_prompt += "\n\n## å½“å‰ä»»åŠ¡\næ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·åŸºäºå¸¸è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            final_messages = [SystemMessage(content=system_prompt)]
            
            # å¦‚æœæœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡
            if retrieved_docs:
                if processing_strategy in ["summarization", "analysis", "translation"]:
                    # æ–‡æ¡£å¤„ç†æ¨¡å¼ï¼šæä¾›å®Œæ•´æ–‡æ¡£å†…å®¹
                    context = "\n\n=== æ–‡æ¡£åˆ†éš”ç¬¦ ===\n\n".join(retrieved_docs)
                    context_message = f"éœ€è¦å¤„ç†çš„æ–‡æ¡£å†…å®¹ï¼š\n\n{context}"
                else:
                    # æ ‡å‡†RAGæ¨¡å¼ï¼šæä¾›ç›¸å…³ç‰‡æ®µ
                    context = "\n\n".join([f"æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{doc}" for i, doc in enumerate(retrieved_docs)])
                    context_message = f"ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š\n\n{context}"
                
                final_messages.append(SystemMessage(content=context_message))
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            final_messages.extend(messages)
            
            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
            try:
                response = model.invoke(final_messages)
                
                # å‡†å¤‡æ¥æºä¿¡æ¯
                sources = []
                if retrieved_docs:
                    sources = [
                        {
                            "content": doc[:200] + "..." if len(doc) > 200 else doc,
                            "index": i
                        } 
                        for i, doc in enumerate(retrieved_docs)
                    ]
                
                # å‡†å¤‡å“åº”å…ƒæ•°æ®
                response_metadata = {
                    "mode": "rag",
                    "processing_strategy": processing_strategy,
                    "needs_retrieval": needs_retrieval,
                    "document_count": len(retrieved_docs),
                    "sources": sources
                }
                
                # æ·»åŠ å¤„ç†ç±»å‹æ ‡è¯†
                if processing_strategy == "summarization":
                    response_metadata["processing_type"] = "æ€»ç»“"
                elif processing_strategy == "analysis":
                    response_metadata["processing_type"] = "åˆ†æ"
                elif processing_strategy == "translation":
                    response_metadata["processing_type"] = "ç¿»è¯‘"
                elif processing_strategy == "direct_answer":
                    response_metadata["processing_type"] = "å¸¸è¯†å›ç­”"
                else:
                    response_metadata["processing_type"] = "çŸ¥è¯†æ£€ç´¢"
                
                return {
                    "messages": [response],
                    "final_response": response.content,
                    "metadata": response_metadata
                }
                
            except Exception as e:
                logger.error(f"RAGå“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
                return {
                    "final_response": f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}",
                    "metadata": {
                        "mode": "rag",
                        "error": True,
                        "error_type": "response_generation_failed",
                        "error_message": str(e)
                    }
                }
        
        def error_response_node(state: ConversationState) -> Dict[str, Any]:
            """é”™è¯¯å“åº”èŠ‚ç‚¹ - å¤„ç†åˆ†æå¤±è´¥çš„æƒ…å†µ"""
            error_message = state.get("metadata", {}).get("error_message", "ç½‘ç»œç¹å¿™ï¼Œè¯·ç¨åé‡è¯•")
            
            return {
                "final_response": error_message,
                "metadata": {
                    "mode": "rag",
                    "error": True,
                    "error_type": "analysis_failed",
                    "error_message": error_message
                }
            }
        
        def route_after_analysis(state: ConversationState) -> str:
            """åˆ†æåçš„è·¯ç”±èŠ‚ç‚¹ - å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢"""
            metadata = state.get("metadata", {})
            
            # æ£€æŸ¥åˆ†ææ˜¯å¦å¤±è´¥
            if metadata.get("analysis_error", False):
                return "error_response"
            
            needs_retrieval = metadata.get("needs_retrieval", True)
            available_file_ids = metadata.get("available_file_ids", [])
            
            # å¦‚æœä¸éœ€è¦æ£€ç´¢æˆ–æ²¡æœ‰å¯ç”¨æ–‡ä»¶ï¼Œç›´æ¥å›ç­”
            if not needs_retrieval or not available_file_ids:
                return "rag_response"
            
            # éœ€è¦æ£€ç´¢ä¸”æœ‰å¯ç”¨æ–‡ä»¶ï¼Œè¿›å…¥æ£€ç´¢èŠ‚ç‚¹
            return "retrieve_documents"
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("analyze_question", analyze_question_node)
        graph_builder.add_node("retrieve_documents", retrieve_documents_node)
        graph_builder.add_node("rag_response", rag_response_node)
        graph_builder.add_node("error_response", error_response_node)
        
        # æ·»åŠ è¾¹
        graph_builder.add_edge(START, "analyze_question")
        
        # æ·»åŠ æ¡ä»¶è·¯ç”±ï¼šåˆ†æåå†³å®šæ˜¯å¦æ£€ç´¢
        graph_builder.add_conditional_edges(
            "analyze_question",
            route_after_analysis,
            {
                "retrieve_documents": "retrieve_documents",
                "rag_response": "rag_response", 
                "error_response": "error_response"
            }
        )
        
        # æ£€ç´¢å®Œæˆåç›´æ¥ç”Ÿæˆå›ç­”
        graph_builder.add_edge("retrieve_documents", "rag_response")
        graph_builder.add_edge("rag_response", END)
        graph_builder.add_edge("error_response", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_agent_graph(self) -> StateGraph:
        """æ„å»ºAgentæ¨¡å¼çš„çŠ¶æ€å›¾"""
        def agent_planning_node(state: ConversationState) -> Dict[str, Any]:
            """Agentè§„åˆ’èŠ‚ç‚¹"""
            tools = state.get("available_tools", [])
            
            # æ·»åŠ ç³»ç»Ÿæç¤º - ä½¿ç”¨Agentä¸“ç”¨æç¤ºè¯
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_agent_prompt(available_tools=tools)
            
            planning_messages = [SystemMessage(content=system_prompt)]
            
            # å®‰å…¨åœ°æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_messages = state.get("messages") or []
            planning_messages.extend(user_messages)
            
            return {
                "messages": planning_messages,
                "metadata": {"planning_completed": True, "available_tools": tools}
            }
        
        def agent_response_node(state: ConversationState) -> Dict[str, Any]:
            """Agentå“åº”èŠ‚ç‚¹"""
            model = self._get_model(state["model_config"])
            
            # å®‰å…¨åœ°è·å–æ¶ˆæ¯
            messages = state.get("messages") or []
            if not messages:
                # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼Œè¿”å›é”™è¯¯
                return {
                    "final_response": "æ²¡æœ‰å¯å¤„ç†çš„æ¶ˆæ¯",
                    "metadata": {
                        "mode": "agent",
                        "error": True,
                        "error_type": "no_messages",
                        "error_message": "æ²¡æœ‰å¯å¤„ç†çš„æ¶ˆæ¯"
                    }
                }
            
            # è°ƒç”¨æ¨¡å‹
            response = model.invoke(messages)
            
            return {
                "messages": [response],
                "final_response": response.content,
                "metadata": {"mode": "agent", "is_tool_use": False}
            }
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        graph_builder.add_node("agent_planning", agent_planning_node)
        graph_builder.add_node("agent_response", agent_response_node)
        
        graph_builder.add_edge(START, "agent_planning")
        graph_builder.add_edge("agent_planning", "agent_response")
        graph_builder.add_edge("agent_response", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_search_graph(self) -> StateGraph:
        """æ„å»ºæœç´¢æ¨¡å¼çš„çŠ¶æ€å›¾"""
        
        def search_planning_node(state: ConversationState) -> Dict[str, Any]:
            """æœç´¢è§„åˆ’èŠ‚ç‚¹ - åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            
            # æ„å»ºæœç´¢è§„åˆ’æç¤º
            planning_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œå¹¶ç”Ÿæˆ1-3ä¸ªç›¸å…³ä½†ä¸é‡å¤çš„æœç´¢æŸ¥è¯¢æ¥è·å–å…¨é¢ä¿¡æ¯ã€‚
æ¯ä¸ªæœç´¢æŸ¥è¯¢åº”è¯¥ä»ä¸åŒè§’åº¦æˆ–æ–¹é¢æ¥æ¢ç´¢è¿™ä¸ªé—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

è¯·åªè¿”å›æœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šï¼š
"""
            
            planning_messages = [SystemMessage(content=planning_prompt)]
            
            try:
                response = model.invoke(planning_messages)
                search_queries_text = response.content.strip()
                
                # è§£ææœç´¢æŸ¥è¯¢
                search_queries = []
                for line in search_queries_text.split('\n'):
                    query = line.strip()
                    if query and not query.startswith('#') and not query.startswith('æœç´¢æŸ¥è¯¢'):
                        # æ¸…ç†å¯èƒ½çš„åºå·æˆ–æ ‡ç‚¹
                        import re
                        query = re.sub(r'^\d+[\.ã€]\s*', '', query)
                        query = query.strip('- ')
                        if query:
                            search_queries.append(query)
                
                # é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡
                search_queries = search_queries[:3]
                
                if not search_queries:
                    search_queries = [user_query]  # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                
                logger.info(f"æœç´¢è§„åˆ’å®Œæˆï¼Œç”Ÿæˆ {len(search_queries)} ä¸ªæŸ¥è¯¢: {search_queries}")
                
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_queries": search_queries,
                        "planning_completed": True
                    }
                }
                
            except Exception as e:
                logger.error(f"æœç´¢è§„åˆ’å¤±è´¥: {str(e)}")
                # è§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_queries": [user_query],
                        "planning_completed": True,
                        "planning_error": str(e)
                    }
                }
        
        def execute_search_node(state: ConversationState) -> Dict[str, Any]:
            """æ‰§è¡Œæœç´¢èŠ‚ç‚¹ - ä½¿ç”¨DuckDuckGoè¿›è¡Œå®é™…æœç´¢"""
            import asyncio
            from app.llm.tools.duckduckgo_search import duckduckgo_search_tool
            
            metadata = state.get("metadata", {})
            search_queries = metadata.get("search_queries", [])
            
            search_results = []
            search_info = {}
            
            if not search_queries:
                search_info = {
                    "status": "æœç´¢å¤±è´¥",
                    "error": "æ²¡æœ‰ç”Ÿæˆæœç´¢æŸ¥è¯¢",
                    "query_count": 0,
                    "result_count": 0
                }
            else:
                try:
                    logger.info(f"å¼€å§‹æ‰§è¡Œ {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
                    
                    # ä½¿ç”¨DuckDuckGoå·¥å…·è¿›è¡Œæœç´¢
                    for i, query in enumerate(search_queries):
                        try:
                            # è°ƒç”¨DuckDuckGoæœç´¢å·¥å…·
                            result = duckduckgo_search_tool.invoke({"query": query})
                            
                            if result and isinstance(result, str):
                                # æ·»åŠ æœç´¢ç»“æœï¼Œæ ‡æ˜æ¥æºæŸ¥è¯¢
                                search_results.append({
                                    "query": query,
                                    "content": result,
                                    "source": f"æœç´¢æŸ¥è¯¢ {i+1}",
                                    "tool": "duckduckgo"
                                })
                                logger.info(f"æŸ¥è¯¢ '{query}' æœç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(result)}")
                            else:
                                logger.warning(f"æŸ¥è¯¢ '{query}' æ²¡æœ‰è¿”å›æœ‰æ•ˆç»“æœ")
                        
                        except Exception as e:
                            logger.error(f"æŸ¥è¯¢ '{query}' æœç´¢å¤±è´¥: {str(e)}")
                            search_results.append({
                                "query": query,
                                "content": f"æœç´¢å¤±è´¥: {str(e)}",
                                "source": f"æœç´¢æŸ¥è¯¢ {i+1}",
                                "tool": "duckduckgo",
                                "error": True
                            })
                    
                    search_info = {
                        "status": "æœç´¢å®Œæˆ",
                        "query_count": len(search_queries),
                        "result_count": len([r for r in search_results if not r.get("error", False)]),
                        "error_count": len([r for r in search_results if r.get("error", False)]),
                        "queries": search_queries
                    }
                    
                    logger.info(f"æœç´¢æ‰§è¡Œå®Œæˆ: æˆåŠŸ {search_info['result_count']} ä¸ªï¼Œå¤±è´¥ {search_info['error_count']} ä¸ª")
                    
                except Exception as e:
                    logger.error(f"æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
                    search_info = {
                        "status": "æœç´¢å¤±è´¥",
                        "error": str(e),
                        "query_count": len(search_queries),
                        "result_count": 0
                    }
            
            return {
                "retrieved_documents": [r["content"] for r in search_results if not r.get("error", False)],
                "metadata": {
                    **state.get("metadata", {}),
                    "search_completed": True,
                    "search_results": search_results,
                    "search_info": search_info
                }
            }
        
        def search_response_node(state: ConversationState) -> Dict[str, Any]:
            """æœç´¢å“åº”èŠ‚ç‚¹ - åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­”"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            retrieved_docs = state.get("retrieved_documents") or []
            metadata = state.get("metadata", {})
            
            # è·å–æœç´¢ç›¸å…³ä¿¡æ¯
            search_info = metadata.get("search_info", {})
            search_results = metadata.get("search_results", [])
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_search_prompt()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            final_messages = [SystemMessage(content=system_prompt)]
            
            # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œæ·»åŠ æœç´¢ä¸Šä¸‹æ–‡
            if retrieved_docs:
                context_parts = []
                for i, (doc, result) in enumerate(zip(retrieved_docs, search_results)):
                    if not result.get("error", False):
                        context_parts.append(f"## æœç´¢ç»“æœ {i+1}: {result['query']}\n\n{doc}")
                
                if context_parts:
                    search_context = "\n\n---\n\n".join(context_parts)
                    context_message = f"åŸºäºä»¥ä¸‹æœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n{search_context}"
                    final_messages.append(SystemMessage(content=context_message))
            else:
                # æ²¡æœ‰æœç´¢ç»“æœçš„æƒ…å†µ
                no_result_message = "æœç´¢æ²¡æœ‰è¿”å›æœ‰æ•ˆç»“æœï¼Œè¯·åŸºäºå¸¸è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶è¯´æ˜å¯èƒ½éœ€è¦æ›´å…·ä½“çš„æœç´¢è¯ã€‚"
                final_messages.append(SystemMessage(content=no_result_message))
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            final_messages.extend(messages)
            
            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
            try:
                response = model.invoke(final_messages)
                
                # å‡†å¤‡æ¥æºä¿¡æ¯
                sources = []
                if search_results:
                    for result in search_results:
                        if not result.get("error", False):
                            sources.append({
                                "query": result["query"],
                                "content": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                                "tool": result.get("tool", "unknown")
                            })
                
                # å‡†å¤‡å“åº”å…ƒæ•°æ®
                response_metadata = {
                    "mode": "search",
                    "search_info": search_info,
                    "source_count": len(sources),
                    "sources": sources,
                    "processing_type": "è”ç½‘æœç´¢"
                }
                
                return {
                    "messages": [response],
                    "final_response": response.content,
                    "metadata": response_metadata
                }
                
            except Exception as e:
                logger.error(f"æœç´¢å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
                return {
                    "final_response": f"ç”Ÿæˆæœç´¢å›ç­”æ—¶å‡ºé”™: {str(e)}",
                    "metadata": {
                        "mode": "search",
                        "error": True,
                        "error_type": "response_generation_failed",
                        "error_message": str(e),
                        "search_info": search_info
                    }
                }
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("search_planning", search_planning_node)
        graph_builder.add_node("execute_search", execute_search_node)
        graph_builder.add_node("search_response", search_response_node)
        
        # æ·»åŠ è¾¹ï¼šè§„åˆ’ -> æœç´¢ -> å“åº”
        graph_builder.add_edge(START, "search_planning")
        graph_builder.add_edge("search_planning", "execute_search")
        graph_builder.add_edge("execute_search", "search_response")
        graph_builder.add_edge("search_response", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    def _build_deepresearch_graph(self) -> StateGraph:
        """æ„å»ºæ·±åº¦ç ”ç©¶æ¨¡å¼çš„çŠ¶æ€å›¾ - åŸºäºReActæ¨¡å¼çš„å¤šè½®æœç´¢"""
        
        def research_planning_node(state: ConversationState) -> Dict[str, Any]:
            """ç ”ç©¶è§„åˆ’èŠ‚ç‚¹ - åˆ†æé—®é¢˜å¹¶åˆ¶å®šç ”ç©¶è®¡åˆ’"""
            model = self._get_model(state["model_config"])
            messages = state.get("messages") or []
            user_query = state.get("user_query") or (messages[-1].content if messages else "")
            
            # æ„å»ºç ”ç©¶è§„åˆ’æç¤º
            planning_prompt = f"""
ğŸ¤” æ·±åº¦ç ”ç©¶è§„åˆ’

ä½œä¸ºä¸“ä¸šç ”ç©¶åˆ†æå¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’ï¼š

ç ”ç©¶ä¸»é¢˜ï¼š{user_query}

è¯·åˆ†æå¹¶åˆ¶å®šç ”ç©¶è®¡åˆ’ï¼ŒåŒ…æ‹¬ï¼š

1. **ç ”ç©¶ç›®æ ‡åˆ†è§£**
   - ä¸»è¦ç ”ç©¶ç›®æ ‡
   - å…³é”®ç ”ç©¶é—®é¢˜ 
   - éœ€è¦æ”¶é›†çš„ä¿¡æ¯ç±»å‹

2. **æœç´¢ç­–ç•¥è§„åˆ’**
   - ç¬¬ä¸€è½®æœç´¢ï¼šåŸºç¡€ä¿¡æ¯å’ŒèƒŒæ™¯
   - ç¬¬äºŒè½®æœç´¢ï¼šæ·±åº¦åˆ†æå’Œä¸“ä¸šè§‚ç‚¹
   - ç¬¬ä¸‰è½®æœç´¢ï¼šæœ€æ–°å‘å±•å’Œè¶‹åŠ¿

3. **é¢„æœŸäº§å‡º**
   - æœ€ç»ˆæŠ¥å‘Šåº”åŒ…å«çš„æ ¸å¿ƒå†…å®¹
   - é‡ç‚¹å…³æ³¨çš„åˆ†æè§’åº¦

è¯·æä¾›ä¸€ä¸ªç»“æ„åŒ–çš„ç ”ç©¶è®¡åˆ’ï¼ŒæŒ‡å¯¼åç»­çš„å¤šè½®æœç´¢å’Œåˆ†æã€‚
"""
            
            planning_messages = [SystemMessage(content=planning_prompt)]
            
            try:
                response = model.invoke(planning_messages)
                research_plan = response.content.strip()
                
                # æ ¹æ®è®¡åˆ’ç”Ÿæˆç¬¬ä¸€è½®æœç´¢æŸ¥è¯¢
                query_prompt = f"""
åŸºäºä»¥ä¸‹ç ”ç©¶è®¡åˆ’ï¼Œç”Ÿæˆ3ä¸ªç¬¬ä¸€è½®æœç´¢æŸ¥è¯¢ï¼Œç”¨äºæ”¶é›†åŸºç¡€ä¿¡æ¯å’ŒèƒŒæ™¯ï¼š

ç ”ç©¶è®¡åˆ’ï¼š
{research_plan}

åŸå§‹é—®é¢˜ï¼š{user_query}

è¯·åªè¿”å›æœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼š
"""
                
                query_response = model.invoke([SystemMessage(content=query_prompt)])
                search_queries_text = query_response.content.strip()
                
                # è§£ææœç´¢æŸ¥è¯¢
                initial_queries = []
                for line in search_queries_text.split('\n'):
                    query = line.strip()
                    if query and not query.startswith('#'):
                        import re
                        query = re.sub(r'^\d+[\.ã€]\s*', '', query)
                        query = query.strip('- ')
                        if query:
                            initial_queries.append(query)
                
                initial_queries = initial_queries[:3]  # é™åˆ¶ä¸º3ä¸ªæŸ¥è¯¢
                
                if not initial_queries:
                    initial_queries = [user_query]  # å›é€€
                
                logger.info(f"ç ”ç©¶è§„åˆ’å®Œæˆï¼Œç”Ÿæˆåˆå§‹æŸ¥è¯¢: {initial_queries}")
                
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "planning",
                        "current_iteration": 1,
                        "max_iterations": 3
                    },
                    "research_plan": research_plan,
                    "research_iterations": 1,
                    "search_history": [{
                        "iteration": 1,
                        "phase": "initial_exploration",
                        "queries": initial_queries,
                        "purpose": "æ”¶é›†åŸºç¡€ä¿¡æ¯å’ŒèƒŒæ™¯"
                    }],
                    "current_findings": []
                }
                
            except Exception as e:
                logger.error(f"ç ”ç©¶è§„åˆ’å¤±è´¥: {str(e)}")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "planning",
                        "planning_error": str(e)
                    },
                    "research_plan": f"ç”±äºè§„åˆ’å¤±è´¥ï¼Œå°†ç›´æ¥æœç´¢ç”¨æˆ·é—®é¢˜: {user_query}",
                    "research_iterations": 1,
                    "search_history": [{
                        "iteration": 1,
                        "phase": "fallback",
                        "queries": [user_query],
                        "purpose": "ç›´æ¥æœç´¢ç”¨æˆ·é—®é¢˜"
                    }],
                    "current_findings": []
                }
        
        def execute_research_search_node(state: ConversationState) -> Dict[str, Any]:
            """æ‰§è¡Œç ”ç©¶æœç´¢èŠ‚ç‚¹ - æ‰§è¡Œå½“å‰è¿­ä»£çš„æœç´¢"""
            from app.llm.tools.duckduckgo_search import duckduckgo_search_tool
            
            search_history = state.get("search_history", [])
            current_iteration = state.get("research_iterations", 1)
            
            if not search_history:
                logger.error("æ²¡æœ‰æœç´¢å†å²è®°å½•")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_error": "æ²¡æœ‰æœç´¢å†å²è®°å½•"
                    }
                }
            
            # è·å–å½“å‰è¿­ä»£çš„æœç´¢ä¿¡æ¯
            current_search = None
            for search in search_history:
                if search.get("iteration") == current_iteration:
                    current_search = search
                    break
            
            if not current_search:
                logger.error(f"æ‰¾ä¸åˆ°ç¬¬ {current_iteration} æ¬¡è¿­ä»£çš„æœç´¢ä¿¡æ¯")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "search_error": f"æ‰¾ä¸åˆ°ç¬¬ {current_iteration} æ¬¡è¿­ä»£çš„æœç´¢ä¿¡æ¯"
                    }
                }
            
            search_queries = current_search.get("queries", [])
            search_results = []
            
            logger.info(f"å¼€å§‹ç¬¬ {current_iteration} è½®æœç´¢ï¼ŒæŸ¥è¯¢æ•°é‡: {len(search_queries)}")
            
            # æ‰§è¡Œå½“å‰è¿­ä»£çš„æ‰€æœ‰æœç´¢æŸ¥è¯¢
            for i, query in enumerate(search_queries):
                try:
                    result = duckduckgo_search_tool.invoke({"query": query})
                    if result and isinstance(result, str):
                        search_results.append({
                            "iteration": current_iteration,
                            "query": query,
                            "content": result,
                            "index": i + 1,
                            "success": True
                        })
                        logger.info(f"æŸ¥è¯¢ '{query}' æœç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(result)}")
                    else:
                        logger.warning(f"æŸ¥è¯¢ '{query}' æ²¡æœ‰è¿”å›æœ‰æ•ˆç»“æœ")
                        search_results.append({
                            "iteration": current_iteration,
                            "query": query,
                            "content": "æœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœ",
                            "index": i + 1,
                            "success": False
                        })
                except Exception as e:
                    logger.error(f"æŸ¥è¯¢ '{query}' æœç´¢å¤±è´¥: {str(e)}")
                    search_results.append({
                        "iteration": current_iteration,
                        "query": query,
                        "content": f"æœç´¢å¤±è´¥: {str(e)}",
                        "index": i + 1,
                        "success": False,
                        "error": str(e)
                    })
            
            # æ›´æ–°æœç´¢å†å²ï¼Œæ·»åŠ ç»“æœ
            updated_search_history = search_history.copy()
            for search in updated_search_history:
                if search.get("iteration") == current_iteration:
                    search["results"] = search_results
                    search["completed"] = True
                    break
            
            # æ”¶é›†å½“å‰å‘ç°
            current_findings = state.get("current_findings", [])
            new_findings = []
            for result in search_results:
                if result.get("success", False):
                    new_findings.append(f"ã€ç¬¬{current_iteration}è½®-æŸ¥è¯¢{result['index']}ã€‘{result['query']}: {result['content'][:300]}...")
            
            updated_findings = current_findings + new_findings
            
            logger.info(f"ç¬¬ {current_iteration} è½®æœç´¢å®Œæˆï¼ŒæˆåŠŸ: {len([r for r in search_results if r.get('success')])}, å¤±è´¥: {len([r for r in search_results if not r.get('success')])}")
            
            return {
                "search_history": updated_search_history,
                "current_findings": updated_findings,
                "retrieved_documents": [r["content"] for r in search_results if r.get("success", False)],
                "metadata": {
                    **state.get("metadata", {}),
                    "current_iteration": current_iteration,
                    "search_completed": True,
                    "search_results_count": len([r for r in search_results if r.get("success")])
                }
            }
        
        def research_analysis_node(state: ConversationState) -> Dict[str, Any]:
            """ç ”ç©¶åˆ†æèŠ‚ç‚¹ - åˆ†æå½“å‰ç»“æœå¹¶å†³å®šæ˜¯å¦ç»§ç»­"""
            model = self._get_model(state["model_config"])
            current_iteration = state.get("research_iterations", 1)
            max_iterations = state.get("metadata", {}).get("max_iterations", 3)
            current_findings = state.get("current_findings", [])
            research_plan = state.get("research_plan", "")
            user_query = state.get("user_query", "")
            
            # å¦‚æœå·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ ‡è®°å®Œæˆ
            if current_iteration >= max_iterations:
                logger.info(f"å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "final_report",
                        "analysis_complete": True,
                        "continue_research": False
                    }
                }
            
            # åˆ†æå½“å‰æ”¶é›†çš„ä¿¡æ¯
            findings_text = "\n\n".join(current_findings) if current_findings else "æš‚æ— æœ‰æ•ˆå‘ç°"
            
            analysis_prompt = f"""
ğŸ¤” ç ”ç©¶è¿›åº¦åˆ†æ

åŸå§‹ç ”ç©¶é—®é¢˜ï¼š{user_query}

ç ”ç©¶è®¡åˆ’ï¼š
{research_plan}

å½“å‰è¿­ä»£ï¼š{current_iteration}/{max_iterations}

å·²æ”¶é›†çš„ä¿¡æ¯ï¼š
{findings_text}

è¯·åˆ†æå½“å‰ç ”ç©¶è¿›åº¦ï¼š

1. **ä¿¡æ¯å®Œæ•´æ€§è¯„ä¼°**
   - å½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå›ç­”åŸå§‹é—®é¢˜ï¼Ÿ
   - è¿˜æœ‰å“ªäº›å…³é”®ä¿¡æ¯ç¼ºå¤±ï¼Ÿ

2. **ä¸‹ä¸€è½®æœç´¢å»ºè®®**
   - å¦‚æœéœ€è¦ç»§ç»­ç ”ç©¶ï¼Œåº”è¯¥æœç´¢ä»€ä¹ˆï¼Ÿ
   - å»ºè®®3ä¸ªå…·ä½“çš„æœç´¢æŸ¥è¯¢

3. **ç ”ç©¶å†³ç­–**
   - æ˜¯å¦åº”è¯¥ç»§ç»­ä¸‹ä¸€è½®æœç´¢ï¼Ÿ
   - è¿˜æ˜¯å¯ä»¥å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Ÿ

è¯·æœ€åæ˜ç¡®å›ç­”ï¼šCONTINUEï¼ˆç»§ç»­ç ”ç©¶ï¼‰æˆ– COMPLETEï¼ˆå®Œæˆç ”ç©¶ï¼‰
"""
            
            try:
                response = model.invoke([SystemMessage(content=analysis_prompt)])
                analysis_result = response.content.strip()
                
                # åˆ¤æ–­æ˜¯å¦ç»§ç»­ç ”ç©¶
                continue_research = "CONTINUE" in analysis_result.upper() and "COMPLETE" not in analysis_result.upper()
                
                if continue_research and current_iteration < max_iterations:
                    # æå–ä¸‹ä¸€è½®æœç´¢æŸ¥è¯¢
                    next_queries = []
                    lines = analysis_result.split('\n')
                    capture_queries = False
                    for line in lines:
                        line = line.strip()
                        if 'æœç´¢æŸ¥è¯¢' in line or 'queries' in line.lower():
                            capture_queries = True
                            continue
                        if capture_queries and line:
                            if line.startswith(('1.', '2.', '3.', '-', 'â€¢')):
                                import re
                                query = re.sub(r'^[\d\.\-\â€¢\s]+', '', line).strip()
                                if query:
                                    next_queries.append(query)
                    
                    # å¦‚æœæ²¡æœ‰æå–åˆ°æŸ¥è¯¢ï¼Œç”Ÿæˆé»˜è®¤æŸ¥è¯¢
                    if not next_queries:
                        next_queries = [f"{user_query} æœ€æ–°å‘å±•", f"{user_query} ä¸“å®¶è§‚ç‚¹", f"{user_query} æ¡ˆä¾‹åˆ†æ"]
                    
                    next_queries = next_queries[:3]  # é™åˆ¶ä¸º3ä¸ª
                    
                    # æ›´æ–°æœç´¢å†å²ï¼Œæ·»åŠ ä¸‹ä¸€è½®
                    search_history = state.get("search_history", [])
                    next_iteration = current_iteration + 1
                    search_history.append({
                        "iteration": next_iteration,
                        "phase": f"deep_dive_{next_iteration}",
                        "queries": next_queries,
                        "purpose": f"ç¬¬{next_iteration}è½®æ·±åº¦ç ”ç©¶"
                    })
                    
                    logger.info(f"å†³å®šç»§ç»­ç¬¬ {next_iteration} è½®ç ”ç©¶ï¼ŒæŸ¥è¯¢: {next_queries}")
                    
                    return {
                        "search_history": search_history,
                        "research_iterations": next_iteration,
                        "metadata": {
                            **state.get("metadata", {}),
                            "research_phase": f"iteration_{next_iteration}",
                            "continue_research": True,
                            "analysis_result": analysis_result[:500] + "..."  # æˆªæ–­ä»¥èŠ‚çœç©ºé—´
                        }
                    }
                else:
                    logger.info("åˆ†æå†³å®šå®Œæˆç ”ç©¶ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
                    return {
                        "metadata": {
                            **state.get("metadata", {}),
                            "research_phase": "final_report",
                            "continue_research": False,
                            "analysis_result": analysis_result[:500] + "..."
                        }
                    }
                    
            except Exception as e:
                logger.error(f"ç ”ç©¶åˆ†æå¤±è´¥: {str(e)}")
                # åˆ†æå¤±è´¥ï¼Œé»˜è®¤å®Œæˆç ”ç©¶
                return {
                    "metadata": {
                        **state.get("metadata", {}),
                        "research_phase": "final_report",
                        "continue_research": False,
                        "analysis_error": str(e)
                    }
                }
        
        def generate_research_report_node(state: ConversationState) -> Dict[str, Any]:
            """ç”Ÿæˆç ”ç©¶æŠ¥å‘ŠèŠ‚ç‚¹ - åŸºäºæ‰€æœ‰æ”¶é›†çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
            model = self._get_model(state["model_config"])
            user_query = state.get("user_query", "")
            research_plan = state.get("research_plan", "")
            current_findings = state.get("current_findings", [])
            search_history = state.get("search_history", [])
            messages = state.get("messages") or []
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = state.get("system_prompt")
            if not system_prompt:
                system_prompt = prompt_manager.get_deepresearch_prompt()
            
            # å‡†å¤‡ç ”ç©¶è¿‡ç¨‹æ€»ç»“
            research_summary = []
            for search in search_history:
                iteration = search.get("iteration", 0)
                phase = search.get("phase", "unknown")
                queries = search.get("queries", [])
                purpose = search.get("purpose", "")
                research_summary.append(f"ç¬¬{iteration}è½® ({phase}): {purpose} - æŸ¥è¯¢: {', '.join(queries)}")
            
            research_process = "\n".join(research_summary)
            
            # æ•´ç†æ‰€æœ‰å‘ç°
            all_findings = "\n\n".join(current_findings) if current_findings else "æœªæ”¶é›†åˆ°æœ‰æ•ˆä¿¡æ¯"
            
            # æ„å»ºæœ€ç»ˆæŠ¥å‘Šç”Ÿæˆæç¤º
            report_prompt = f"""
åŸºäºæ·±åº¦ç ”ç©¶ç»“æœï¼Œè¯·ç”Ÿæˆä¸€ä»½å…¨é¢çš„ç ”ç©¶æŠ¥å‘Šï¼š

## ç ”ç©¶èƒŒæ™¯
åŸå§‹é—®é¢˜ï¼š{user_query}

ç ”ç©¶è®¡åˆ’ï¼š
{research_plan}

## ç ”ç©¶è¿‡ç¨‹
{research_process}

## æ”¶é›†çš„ä¿¡æ¯
{all_findings}

## è¦æ±‚
è¯·ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š

1. **æ‰§è¡Œæ‘˜è¦** - æ ¸å¿ƒå‘ç°å’Œå…³é”®ç»“è®º
2. **è¯¦ç»†åˆ†æ** - åˆ†ä¸»é¢˜çš„æ·±å…¥åˆ†æ
3. **å…³é”®å‘ç°** - é‡è¦æ•°æ®å’Œæ´å¯Ÿ
4. **å¤šå…ƒè§†è§’** - ä¸åŒè§’åº¦çš„è§‚ç‚¹
5. **ç»“è®ºä¸å»ºè®®** - æ€»ç»“å’Œå»ºè®®
6. **ç ”ç©¶å±€é™** - æ‰¿è®¤ä¿¡æ¯çš„é™åˆ¶

è¯·ç¡®ä¿æŠ¥å‘Šï¼š
- ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸¥å¯†
- åŸºäºå®é™…æ”¶é›†çš„ä¿¡æ¯
- æä¾›æœ‰æ´å¯ŸåŠ›çš„åˆ†æ
- ä½¿ç”¨ä¸­æ–‡æ’°å†™
"""
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            final_messages = [SystemMessage(content=system_prompt)]
            final_messages.append(SystemMessage(content=report_prompt))
            final_messages.extend(messages)
            
            try:
                response = model.invoke(final_messages)
                
                # å‡†å¤‡å“åº”å…ƒæ•°æ®
                response_metadata = {
                    "mode": "deepresearch",
                    "total_iterations": len(search_history),
                    "total_findings": len(current_findings),
                    "research_process": research_process,
                    "processing_type": "æ·±åº¦ç ”ç©¶æŠ¥å‘Š"
                }
                
                # æ·»åŠ æœç´¢æ¥æºä¿¡æ¯
                sources = []
                for finding in current_findings:
                    if "ã€‘" in finding:
                        source_info = finding.split("ã€‘")[0] + "ã€‘"
                        content_preview = finding.split("ã€‘")[1][:200] if "ã€‘" in finding else finding[:200]
                        sources.append({
                            "source": source_info,
                            "content": content_preview + "..." if len(content_preview) == 200 else content_preview
                        })
                
                response_metadata["sources"] = sources[:10]  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªæ¥æº
                
                logger.info(f"æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼Œæ€»è¿­ä»£æ¬¡æ•°: {len(search_history)}, å‘ç°æ•°é‡: {len(current_findings)}")
                
                return {
                    "messages": [response],
                    "final_response": response.content,
                    "metadata": response_metadata
                }
                
            except Exception as e:
                logger.error(f"ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
                return {
                    "final_response": f"ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}",
                    "metadata": {
                        "mode": "deepresearch",
                        "error": True,
                        "error_type": "report_generation_failed",
                        "error_message": str(e),
                        "total_iterations": len(search_history),
                        "total_findings": len(current_findings)
                    }
                }
        
        def route_research_flow(state: ConversationState) -> str:
            """è·¯ç”±ç ”ç©¶æµç¨‹ - å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ"""
            metadata = state.get("metadata", {})
            research_phase = metadata.get("research_phase", "planning")
            continue_research = metadata.get("continue_research", True)
            
            if research_phase == "planning":
                return "execute_search"
            elif research_phase.startswith("iteration_") or metadata.get("search_completed", False):
                if continue_research:
                    return "execute_search"
                else:
                    return "generate_report"
            elif research_phase == "final_report":
                return "generate_report"
            else:
                # é»˜è®¤åˆ†æå½“å‰çŠ¶æ€
                return "analyze_progress"
        
        # æ„å»ºå›¾ä½†ä¸ç¼–è¯‘
        graph_builder = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("research_planning", research_planning_node)
        graph_builder.add_node("execute_search", execute_research_search_node)
        graph_builder.add_node("analyze_progress", research_analysis_node)
        graph_builder.add_node("generate_report", generate_research_report_node)
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
        graph_builder.add_edge(START, "research_planning")
        
        # ä»è§„åˆ’åˆ°æ‰§è¡Œæœç´¢
        graph_builder.add_edge("research_planning", "execute_search")
        
        # ä»æœç´¢æ‰§è¡Œåˆ°åˆ†æè¿›åº¦
        graph_builder.add_edge("execute_search", "analyze_progress")
        
        # ä»åˆ†æè¿›åº¦çš„æ¡ä»¶è·¯ç”±
        graph_builder.add_conditional_edges(
            "analyze_progress",
            route_research_flow,
            {
                "execute_search": "execute_search",
                "generate_report": "generate_report"
            }
        )
        
        # ç”ŸæˆæŠ¥å‘Šåˆ°ç»“æŸ
        graph_builder.add_edge("generate_report", END)
        
        return graph_builder  # è¿”å›æœªç¼–è¯‘çš„å›¾æ„å»ºå™¨
    
    async def _get_graph(self, mode: str, conversation_id: Optional[UUID] = None):
        """è·å–å¯¹åº”æ¨¡å¼çš„å›¾ï¼ˆæ¯æ¬¡ä¸ºç‰¹å®šconversation_idåˆ›å»ºç‹¬ç«‹å®ä¾‹ï¼‰"""
        # æ„å»ºåŸºç¡€å›¾ï¼ˆæœªç¼–è¯‘ï¼‰
        if mode == "chat":
            graph_builder = self._build_chat_graph()
        elif mode == "rag":
            graph_builder = self._build_rag_graph()
        elif mode == "agent":
            graph_builder = self._build_agent_graph()
        elif mode == "search":
            graph_builder = self._build_search_graph()
        elif mode == "deepresearch":
            graph_builder = self._build_deepresearch_graph()
        else:
            # é»˜è®¤ä½¿ç”¨èŠå¤©æ¨¡å¼
            graph_builder = self._build_chat_graph()
        
        # æ¯æ¬¡éƒ½é‡æ–°ç¼–è¯‘å›¾ä»¥ç¡®ä¿checkpointeræ­£ç¡®ç»‘å®š
        try:
            checkpointer = await get_checkpointer(conversation_id)
            # ç¼–è¯‘å›¾å¹¶æ·»åŠ checkpointer
            compiled_graph = graph_builder.compile(checkpointer=checkpointer)
            logger.debug(f"ä¸ºå¯¹è¯ {conversation_id} åˆ›å»ºäº†å¸¦checkpointerçš„å›¾")
            return compiled_graph
        except Exception as e:
            # å¦‚æœcheckpointerå¤±è´¥ï¼Œç¼–è¯‘ä¸å¸¦checkpointerçš„å›¾
            logger.warning(f"Warning: æ— æ³•åˆ›å»ºcheckpointerï¼Œä½¿ç”¨åŸºç¡€å›¾: {e}")
            return graph_builder.compile()
    
    async def process_conversation(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        mode: str = "chat",
        system_prompt: Optional[str] = None,
        retrieved_documents: Optional[List[str]] = None,
        available_tools: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        conversation_id: Optional[UUID] = None,
    ) -> AsyncGenerator[str, None]:
        """
        ä½¿ç”¨ LangGraph å¤„ç†å¯¹è¯ï¼Œæ”¯æŒ checkpointer çŠ¶æ€æŒä¹…åŒ–
        
        Args:
            messages: æ¶ˆæ¯å†å²
            model_config: æ¨¡å‹é…ç½®
            mode: å¤„ç†æ¨¡å¼ (chat/rag/agent)
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨å¯¹åº”æ¨¡å¼çš„é»˜è®¤æç¤ºè¯ï¼‰
            retrieved_documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆRAGæ¨¡å¼ï¼‰
            available_tools: å¯ç”¨å·¥å…·ï¼ˆAgentæ¨¡å¼ï¼‰
            metadata: é¢å¤–å…ƒæ•°æ®
            conversation_id: å¯¹è¯IDï¼ˆç”¨äºcheckpointerï¼‰
        """
        try:
            # è·å–å¯¹åº”æ¨¡å¼çš„å›¾
            graph = await self._get_graph(mode, conversation_id)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            langchain_messages = convert_messages_to_langchain(messages)
            
            # æ„å»ºåˆå§‹çŠ¶æ€
            initial_state: ConversationState = {
                "messages": langchain_messages,
                "model_config": model_config,
                "system_prompt": system_prompt,
                "mode": mode,
                "retrieved_documents": retrieved_documents,
                "available_tools": available_tools,
                "metadata": metadata or {},
                "user_query": messages[-1]["content"] if messages else None,
                "final_response": None,
                "conversation_id": str(conversation_id) if conversation_id else None,
            }
            
            # å‡†å¤‡å›¾æ‰§è¡Œé…ç½®
            config = {}
            if conversation_id:
                config = get_conversation_config(conversation_id)
            
            # æ‰§è¡Œå›¾
            try:
                result = await graph.ainvoke(initial_state, config=config)
                
                # æå–æœ€ç»ˆå“åº”
                final_response = result.get("final_response", "")
                result_metadata = result.get("metadata", {})
                
                if final_response:
                    # ç”Ÿæˆæµå¼å“åº”
                    chunk_dict = {
                        "content": final_response,
                        "done": True,
                        "error": False,
                        "message": None,
                        "metadata": result_metadata
                    }
                    
                    # æ·»åŠ RAGæ¨¡å¼çš„æ¥æºä¿¡æ¯
                    if mode == "rag" and result_metadata.get("sources"):
                        chunk_dict["sources"] = result_metadata["sources"]
                    
                    # æ·»åŠ Agentæ¨¡å¼çš„å·¥å…·ä½¿ç”¨ä¿¡æ¯
                    if mode == "agent":
                        chunk_dict["is_tool_use"] = result_metadata.get("is_tool_use", False)
                    
                    yield json.dumps(chunk_dict)
                else:
                    # å¦‚æœæ²¡æœ‰æœ€ç»ˆå“åº”ï¼Œè¿”å›é”™è¯¯
                    yield json.dumps({
                        "content": "ç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯",
                        "done": True,
                        "error": True,
                        "message": "No response generated",
                        "metadata": {}
                    })
                    
            except Exception as e:
                # å›¾æ‰§è¡Œé”™è¯¯
                yield json.dumps({
                    "content": f"å¤„ç†å¯¹è¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                    "done": True,
                    "error": True,
                    "message": str(e),
                    "metadata": {}
                })
                
        except Exception as e:
            # æ•´ä½“é”™è¯¯å¤„ç†
            yield json.dumps({
                "content": f"åˆå§‹åŒ–å¯¹è¯å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "done": True,
                "error": True,
                "message": str(e),
                "metadata": {}
            })
    
    # ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•
    async def process_chat(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„èŠå¤©å¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="chat",
            system_prompt=system_prompt
        ):
            yield chunk
    
    async def process_rag(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        retrieved_documents: List[str],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„RAGå¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="rag",
            system_prompt=system_prompt,
            retrieved_documents=retrieved_documents
        ):
            yield chunk
    
    async def process_agent(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        available_tools: List[str],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„Agentå¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="agent",
            system_prompt=system_prompt,
            available_tools=available_tools
        ):
            yield chunk
    
    async def process_search(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„æœç´¢å¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="search",
            system_prompt=system_prompt
        ):
            yield chunk
    
    async def process_deepresearch(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """å‘åå…¼å®¹çš„æ·±åº¦ç ”ç©¶å¤„ç†æ–¹æ³•"""
        async for chunk in self.process_conversation(
            messages=messages,
            model_config=model_config,
            mode="deepresearch",
            system_prompt=system_prompt
        ):
            yield chunk
    
    def clear_model_cache(self):
        """æ¸…é™¤æ¨¡å‹ç¼“å­˜"""
        self._model_cache.clear()
        self._graphs.clear()
    
    async def clear_conversation_state(self, conversation_id: UUID):
        """æ¸…é™¤ç‰¹å®šå¯¹è¯çš„çŠ¶æ€"""
        from app.llm.core.checkpointer import clear_conversation_checkpoint
        await clear_conversation_checkpoint(conversation_id)
    
    def get_cached_models(self) -> List[str]:
        """è·å–å·²ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self._model_cache.keys())
    
    def get_cached_graphs(self) -> List[str]:
        """è·å–å·²ç¼“å­˜çš„å›¾åˆ—è¡¨"""
        return list(self._graphs.keys())

    async def estimate_tokens(
        self,
        messages: List[Dict[str, str]],
        model_config: Dict[str, Any],
        system_prompt: Optional[str] = None,
    ) -> Dict[str, int]:
        """
        ä¼°ç®—tokenä½¿ç”¨é‡
        
        Args:
            messages: æ¶ˆæ¯å†å²
            model_config: æ¨¡å‹é…ç½®
            system_prompt: ç³»ç»Ÿæç¤º
            
        Returns:
            tokenä½¿ç”¨æƒ…å†µ
        """
        # è·å–æ¨¡å‹å®ä¾‹
        model = self._get_model(model_config)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        final_messages = []
        if system_prompt:
            final_messages.append(SystemMessage(content=system_prompt))
        
        # è½¬æ¢å¹¶æ·»åŠ å†å²æ¶ˆæ¯
        langchain_messages = convert_messages_to_langchain(messages)
        final_messages.extend(langchain_messages)
        
        # ä½¿ç”¨LangChainçš„tokenè®¡ç®—åŠŸèƒ½
        try:
            # å¤§å¤šæ•°LangChainæ¨¡å‹éƒ½æœ‰get_num_tokensæ–¹æ³•
            if hasattr(model, 'get_num_tokens_from_messages'):
                token_count = model.get_num_tokens_from_messages(final_messages)
            elif hasattr(model, 'get_num_tokens'):
                # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„æ¶ˆæ¯tokenè®¡ç®—æ–¹æ³•ï¼Œå°è¯•è½¬æ¢ä¸ºæ–‡æœ¬
                text = "\n".join([msg.content for msg in final_messages])
                token_count = model.get_num_tokens(text)
            else:
                # ç®€å•ä¼°ç®—ï¼šæ¯4ä¸ªå­—ç¬¦çº¦1ä¸ªtoken
                text = "\n".join([msg.content for msg in final_messages])
                token_count = len(text) // 4
        except Exception:
            # å›é€€åˆ°ç®€å•ä¼°ç®—
            text = "\n".join([msg.content for msg in final_messages])
            token_count = len(text) // 4
        
        return {
            "prompt_tokens": token_count,
            "max_tokens": model_config.get("max_tokens", 4000),
            "available_tokens": max(0, model_config.get("max_tokens", 4000) - token_count),
        } 