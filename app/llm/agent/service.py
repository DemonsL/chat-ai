import json
from typing import Any, AsyncGenerator, Dict, List, Optional
from uuid import UUID

from langchain.agents import AgentType, initialize_agent
from langchain.agents.agent import AgentExecutor
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.tools import BaseTool
from langchain.tools.base import tool
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from app.core.config import settings
from app.core.exceptions import LLMAPIException, NotFoundException
from app.db.repositories.conversation_repository import ConversationRepository
from app.db.repositories.message_repository import MessageRepository
from app.db.repositories.model_config_repository import ModelConfigRepository
from app.llm.core.base import Message


class AgentCallbackHandler(BaseCallbackHandler):
    """Agentå›žè°ƒå¤„ç†å™¨ï¼Œç”¨äºŽå°†Agentæ‰§è¡Œè¿‡ç¨‹è½¬æ¢ä¸ºæµå¼è¾“å‡º"""

    def __init__(self):
        self.tokens = []
        self.done = False
        self.intermediate_steps = []

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs
    ) -> None:
        """å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶çš„å›žè°ƒ"""
        tool_name = serialized.get("name", "unknown_tool")
        self.intermediate_steps.append(f"ðŸ”§ ä½¿ç”¨å·¥å…·: {tool_name}\nè¾“å…¥: {input_str}\n")

    def on_tool_end(self, output: str, **kwargs) -> None:
        """å·¥å…·æ‰§è¡Œç»“æŸæ—¶çš„å›žè°ƒ"""
        self.intermediate_steps.append(f"ðŸ”§ å·¥å…·è¾“å‡º: {output}\n\n")

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs
    ) -> None:
        """LLMå¼€å§‹ç”Ÿæˆæ—¶çš„å›žè°ƒ"""
        pass

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """LLMç”Ÿæˆæ–°tokenæ—¶çš„å›žè°ƒ"""
        self.tokens.append(token)

    def on_llm_end(self, response, **kwargs) -> None:
        """LLMç”Ÿæˆç»“æŸæ—¶çš„å›žè°ƒ"""
        self.done = True

    def on_agent_action(self, action, **kwargs) -> None:
        """Agentæ‰§è¡ŒåŠ¨ä½œæ—¶çš„å›žè°ƒ"""
        self.intermediate_steps.append(f"ðŸ¤– æ€è€ƒ: æˆ‘éœ€è¦ä½¿ç”¨ {action.tool} å·¥å…·\n\n")

    def on_agent_finish(self, finish, **kwargs) -> None:
        """Agentç»“æŸæ—¶çš„å›žè°ƒ"""
        self.intermediate_steps.append(
            f"âœ… å®Œæˆ: {finish.return_values.get('output')}\n"
        )
        self.done = True


class AgentService:
    """
    AgentæœåŠ¡ï¼Œæä¾›æ·±åº¦ç ”ç©¶èƒ½åŠ›
    """

    def __init__(
        self,
        message_repo: MessageRepository,
        conversation_repo: ConversationRepository,
        model_repo: ModelConfigRepository,
    ):
        self.message_repo = message_repo
        self.conversation_repo = conversation_repo
        self.model_repo = model_repo

    async def process_message(
        self, conversation_id: UUID, content: str, metadata: Optional[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶æ‰§è¡Œæ·±åº¦ç ”ç©¶

        å‚æ•°:
            conversation_id: ä¼šè¯ID
            content: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            metadata: å¯é€‰çš„å…ƒæ•°æ®

        è¿”å›ž:
            æ‰§è¡Œç»“æžœçš„æµå¼ç”Ÿæˆå™¨
        """
        # èŽ·å–ä¼šè¯ä¿¡æ¯
        conversation = await self.conversation_repo.get_by_id(conversation_id)
        if not conversation:
            raise NotFoundException(detail="ä¼šè¯ä¸å­˜åœ¨")

        # èŽ·å–æ¨¡åž‹é…ç½®
        model_config = await self.model_repo.get_by_model_id(conversation.model_id)
        if not model_config or not model_config.is_active:
            raise NotFoundException(detail="æ‰€é€‰æ¨¡åž‹ä¸å¯ç”¨")

        # æ£€æŸ¥æ¨¡åž‹æ˜¯å¦æ”¯æŒAgentèƒ½åŠ›
        if "agent" not in model_config.capabilities:
            raise LLMAPIException(detail="æ‰€é€‰æ¨¡åž‹ä¸æ”¯æŒæ·±åº¦ç ”ç©¶åŠŸèƒ½")

        # èŽ·å–åŽ†å²æ¶ˆæ¯
        history = await self.message_repo.get_conversation_history(conversation_id)

        # åˆ›å»ºLLM
        llm = ChatOpenAI(
            model=model_config.model_id,
            openai_api_key=settings.OPENAI_API_KEY,
            temperature=0.7,
            streaming=True,
        )

        # åˆ›å»ºå·¥å…·
        tools = self._get_tools()

        # åˆ›å»ºå›žè°ƒå¤„ç†å™¨
        callback_handler = AgentCallbackHandler()

        # åˆ›å»ºè®°å¿†
        memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )

        # æ·»åŠ åŽ†å²æ¶ˆæ¯åˆ°è®°å¿†
        for msg in history:
            role = "human" if msg.role == "user" else "ai"
            memory.chat_memory.add_message({"role": role, "content": msg.content})

        # åˆ›å»ºAgent
        agent = initialize_agent(
            tools=tools,
            llm=llm,
            agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
            memory=memory,
            verbose=True,
            handle_parsing_errors=True,
        )

        # åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯
        system_message = (
            conversation.system_prompt
            or "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å›žç­”é—®é¢˜å’Œè§£å†³é—®é¢˜ã€‚"
        )

        # å¯åŠ¨Agentæ‰§è¡Œ
        try:
            # ä½¿ç”¨å¼‚æ­¥è¿è¡Œ
            # æ³¨æ„ï¼šLangChainç›®å‰çš„Agentæ‰§è¡Œä¸æ˜¯å®Œå…¨å¼‚æ­¥çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å¤„ç†
            # ä¸ºäº†çœŸæ­£çš„æµå¼å¤„ç†ï¼Œå¯èƒ½éœ€è¦è‡ªå®šä¹‰å®žçŽ°
            agent.run(
                input={
                    "input": content,
                    "chat_history": memory.chat_memory.messages,
                },
                callbacks=[callback_handler],
            )

            # ç”Ÿæˆæµå¼å“åº”
            while (
                not callback_handler.done
                or callback_handler.tokens
                or callback_handler.intermediate_steps
            ):
                # è¾“å‡ºä¸­é—´æ­¥éª¤
                if callback_handler.intermediate_steps:
                    step = callback_handler.intermediate_steps.pop(0)
                    yield json.dumps(
                        {"content": step, "done": False, "is_tool_use": True}
                    )

                # è¾“å‡ºLLM token
                if callback_handler.tokens:
                    token = callback_handler.tokens.pop(0)
                    yield json.dumps({"content": token, "done": False})

                # å¦‚æžœæ²¡æœ‰æ›´å¤šå†…å®¹ä½†è¿˜æ²¡å®Œæˆï¼Œç­‰å¾…ä¸€ä¸‹
                if (
                    not callback_handler.intermediate_steps
                    and not callback_handler.tokens
                    and not callback_handler.done
                ):
                    import asyncio

                    await asyncio.sleep(0.1)

            # å‘é€å®Œæˆæ ‡è®°
            yield json.dumps({"content": "", "done": True})

        except Exception as e:
            yield json.dumps(
                {
                    "content": f"æ‰§è¡Œæ·±åº¦ç ”ç©¶æ—¶å‡ºé”™: {str(e)}",
                    "done": True,
                    "error": True,
                }
            )

    def _get_tools(self) -> List[BaseTool]:
        """èŽ·å–Agentå¯ç”¨çš„å·¥å…·"""

        @tool("search", return_direct=False)
        def search(query: str) -> str:
            """æœç´¢äº’è”ç½‘ä»¥æŸ¥æ‰¾æœ‰å…³ç‰¹å®šä¸»é¢˜çš„ä¿¡æ¯ã€‚"""
            # è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå®žçŽ°ï¼Œå®žé™…åº”ä½¿ç”¨çœŸå®žçš„æœç´¢API
            return f"æ¨¡æ‹Ÿæœç´¢ç»“æžœ: {query}"

        @tool("calculator", return_direct=False)
        def calculator(expression: str) -> str:
            """è¿›è¡Œæ•°å­¦è®¡ç®—ã€‚"""
            try:
                return str(eval(expression))
            except Exception as e:
                return f"è®¡ç®—é”™è¯¯: {str(e)}"

        # è¿”å›žå·¥å…·åˆ—è¡¨
        return [search, calculator]
