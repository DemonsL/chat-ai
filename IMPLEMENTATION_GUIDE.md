# LLMæ¨¡å—ä¼˜åŒ–å®æ–½æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨é€æ­¥å®æ–½LLMæ¨¡å—çš„ä¼˜åŒ–æ”¹è¿›ï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡å’Œæœ€å°åŒ–é£é™©ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®æ¡ä»¶

1. **ç³»ç»Ÿè¦æ±‚**
   - Docker & Docker Compose
   - Python 3.11+
   - è‡³å°‘16GB RAM
   - 100GBå¯ç”¨ç£ç›˜ç©ºé—´

2. **ä¾èµ–å®‰è£…**
   ```bash
   # å®‰è£…æ–°çš„ä¾èµ–åŒ…
   pip install rank-bm25 sentence-transformers jieba
   pip install prometheus-client opentelemetry-api
   pip install elasticsearch kibana
   ```

## ğŸ“… åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ä¼˜åŒ– (ç¬¬1-2å‘¨)

#### 1.1 éƒ¨ç½²æ„å›¾è·Ÿè¸ªå™¨

```bash
# 1. å¤åˆ¶æ„å›¾è·Ÿè¸ªå™¨æ–‡ä»¶
cp app/llm/enhance/intent_tracker.py app/llm/enhance/

# 2. åœ¨LLMç®¡ç†å™¨ä¸­é›†æˆ
# ç¼–è¾‘ app/llm/manage.pyï¼Œæ·»åŠ æ„å›¾è·Ÿè¸ªåŠŸèƒ½
```

**é›†æˆä»£ç ç¤ºä¾‹**ï¼š
```python
# åœ¨ LLMManager.__init__ ä¸­æ·»åŠ 
from app.llm.enhance.intent_tracker import IntentTracker

def __init__(self, retrieval_service=None):
    # ... ç°æœ‰ä»£ç  ...
    self.intent_tracker = IntentTracker()
    
# åœ¨å¯¹è¯å¤„ç†ä¸­ä½¿ç”¨
async def process_conversation(self, messages, ...):
    # è·Ÿè¸ªæ„å›¾
    if len(messages) > 1:
        intent_update = await self.intent_tracker.track_intent(
            current_message=messages[-1]["content"],
            conversation_history=convert_messages_to_langchain(messages[:-1])
        )
        metadata["intent_analysis"] = intent_update
```

#### 1.2 éƒ¨ç½²è´¨é‡è¯„ä¼°å™¨

```bash
# 1. å¤åˆ¶è´¨é‡è¯„ä¼°å™¨æ–‡ä»¶
cp app/llm/enhance/quality_evaluator.py app/llm/enhance/

# 2. åˆ›å»ºè´¨é‡è¯„ä¼°APIç«¯ç‚¹
```

**APIé›†æˆç¤ºä¾‹**ï¼š
```python
# åœ¨ app/api/v1/endpoints/ ä¸­åˆ›å»ºæ–°ç«¯ç‚¹
from app.llm.enhance.quality_evaluator import ConversationQualityEvaluator

@router.post("/evaluate-quality")
async def evaluate_quality(
    query: str,
    response: str,
    context: Dict = None
):
    evaluator = ConversationQualityEvaluator()
    report = await evaluator.evaluate_response_quality(query, response, context)
    return report
```

#### 1.3 é…ç½®ç›‘æ§ç³»ç»Ÿ

```bash
# 1. åˆ›å»ºç›‘æ§é…ç½®ç›®å½•
mkdir -p monitoring/grafana/{dashboards,datasources}
mkdir -p monitoring/prometheus

# 2. é…ç½®Prometheus
cat > monitoring/prometheus.yml << EOF
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'llm-api'
    static_configs:
      - targets: ['llm-api:8000']
    metrics_path: '/metrics'
    
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']
      
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
EOF

# 3. å¯åŠ¨åŸºç¡€ç›‘æ§
docker-compose -f docker-compose.enhanced.yml up -d prometheus grafana
```

### ç¬¬äºŒé˜¶æ®µï¼šæ£€ç´¢å¢å¼º (ç¬¬3-4å‘¨)

#### 2.1 éƒ¨ç½²æ··åˆæ£€ç´¢å™¨

```bash
# 1. å¤åˆ¶æ··åˆæ£€ç´¢å™¨æ–‡ä»¶
cp app/llm/enhance/hybrid_retriever.py app/llm/enhance/

# 2. å®‰è£…BM25ä¾èµ–
pip install rank-bm25

# 3. é…ç½®Elasticsearch
docker-compose -f docker-compose.enhanced.yml up -d elasticsearch kibana
```

**é›†æˆæ··åˆæ£€ç´¢**ï¼š
```python
# åœ¨æ£€ç´¢æœåŠ¡ä¸­é›†æˆ
from app.llm.enhance.hybrid_retriever import HybridRetriever, SearchConfig

class LLMRetrievalService:
    def __init__(self):
        # ... ç°æœ‰ä»£ç  ...
        self.hybrid_retriever = HybridRetriever(
            vector_store=self.vector_store,
            config=SearchConfig(
                strategy="hybrid",
                vector_weight=0.7,
                keyword_weight=0.3
            )
        )
    
    async def retrieve_documents(self, query: str, **kwargs):
        # ä½¿ç”¨æ··åˆæ£€ç´¢
        results = await self.hybrid_retriever.search(query, **kwargs)
        return [result.document for result in results]
```

#### 2.2 ä¼˜åŒ–æ–‡æ¡£åˆ†å—

```python
# åˆ›å»ºè‡ªé€‚åº”åˆ†å—å™¨
class AdaptiveDocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", " "]
        )
        
    def process_document(self, document: Document) -> List[Document]:
        # æ£€æµ‹æ–‡æ¡£ç±»å‹
        doc_type = self._detect_document_type(document.page_content)
        
        if doc_type == "code":
            return self._split_code_document(document)
        elif doc_type == "academic":
            return self._split_academic_document(document)
        else:
            return self.text_splitter.split_documents([document])
```

### ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ– (ç¬¬5-6å‘¨)

#### 3.1 å®æ–½å¤šçº§ç¼“å­˜

```python
# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
import redis
from functools import lru_cache

class CacheManager:
    def __init__(self):
        self.redis_client = redis.Redis.from_url("redis://redis:6379")
        self.memory_cache = {}
        
    @lru_cache(maxsize=1000)
    def get_embedding_cache(self, text_hash: str):
        """å†…å­˜ç¼“å­˜åµŒå…¥å‘é‡"""
        return self.redis_client.get(f"embedding:{text_hash}")
    
    async def cache_response(self, query_hash: str, response: str):
        """ç¼“å­˜å“åº”ç»“æœ"""
        # L1: å†…å­˜ç¼“å­˜
        self.memory_cache[query_hash] = response
        
        # L2: Redisç¼“å­˜
        await self.redis_client.setex(
            f"response:{query_hash}", 
            3600,  # 1å°æ—¶TTL
            response
        )
```

#### 3.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡åµŒå…¥å¤„ç†
class BatchEmbeddingProcessor:
    def __init__(self, batch_size: int = 32):
        self.batch_size = batch_size
        self.embedding_queue = asyncio.Queue()
        
    async def batch_embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        embeddings = []
        
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_embeddings = await self._process_batch(batch)
            embeddings.extend(batch_embeddings)
            
        return embeddings
    
    async def _process_batch(self, batch: List[str]) -> List[List[float]]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        # ä½¿ç”¨embeddingæœåŠ¡çš„æ‰¹é‡API
        return await self.embedding_service.embed_documents(batch)
```

### ç¬¬å››é˜¶æ®µï¼šä¼ä¸šçº§éƒ¨ç½² (ç¬¬7-8å‘¨)

#### 4.1 å¾®æœåŠ¡æ¶æ„æ”¹é€ 

```bash
# 1. åˆ›å»ºç‹¬ç«‹çš„å¾®æœåŠ¡
mkdir -p services/{quality-evaluator,intent-tracker,hybrid-retriever}

# 2. ä¸ºæ¯ä¸ªæœåŠ¡åˆ›å»ºDockerfile
cat > services/quality-evaluator/Dockerfile << EOF
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY app/llm/enhance/quality_evaluator.py .
COPY service_main.py .

CMD ["python", "service_main.py"]
EOF

# 3. åˆ›å»ºæœåŠ¡æ³¨å†Œä¸­å¿ƒ
# ä½¿ç”¨Consulæˆ–etcdè¿›è¡ŒæœåŠ¡å‘ç°
```

#### 4.2 å®¹å™¨åŒ–éƒ¨ç½²

```bash
# 1. æ„å»ºæ‰€æœ‰æœåŠ¡é•œåƒ
docker-compose -f docker-compose.enhanced.yml build

# 2. å¯åŠ¨å®Œæ•´ç³»ç»Ÿ
docker-compose -f docker-compose.enhanced.yml up -d

# 3. éªŒè¯æœåŠ¡çŠ¶æ€
docker-compose -f docker-compose.enhanced.yml ps
```

## ğŸ”§ é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env.production
POSTGRES_URL=postgresql://chatai:chatai123@postgres:5432/chatai
REDIS_URL=redis://redis:6379
CHROMA_URL=http://chroma:8000
ELASTICSEARCH_URL=http://elasticsearch:9200

# åŠŸèƒ½å¼€å…³
ENABLE_QUALITY_EVALUATION=true
ENABLE_INTENT_TRACKING=true
ENABLE_HYBRID_RETRIEVAL=true
ENABLE_CACHING=true

# æ€§èƒ½é…ç½®
MAX_CONCURRENT_REQUESTS=100
CACHE_TTL=3600
BATCH_SIZE=32
```

### Nginxè´Ÿè½½å‡è¡¡é…ç½®

```nginx
# nginx/nginx.conf
upstream llm_api {
    least_conn;
    server llm-api:8000 max_fails=3 fail_timeout=30s;
    server llm-api:8000 max_fails=3 fail_timeout=30s;
    server llm-api:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://llm_api;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 30s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    location /health {
        access_log off;
        proxy_pass http://llm_api/health;
    }
}
```

## ğŸ“Š ç›‘æ§ä¸å‘Šè­¦

### Grafanaä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "LLM System Monitoring",
    "panels": [
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Quality Scores",
        "type": "stat",
        "targets": [
          {
            "expr": "avg(quality_score_overall)",
            "legendFormat": "Average Quality"
          }
        ]
      }
    ]
  }
}
```

### å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# monitoring/alert-rules.yml
groups:
  - name: llm_system_alerts
    rules:
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          
      - alert: LowQualityScore
        expr: avg(quality_score_overall) < 0.7
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Quality score below threshold"
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### åŠŸèƒ½æµ‹è¯•

```python
# tests/test_optimizations.py
import pytest
from app.llm.enhance.intent_tracker import IntentTracker
from app.llm.enhance.quality_evaluator import ConversationQualityEvaluator

@pytest.mark.asyncio
async def test_intent_tracking():
    tracker = IntentTracker()
    
    # æµ‹è¯•æ„å›¾è·Ÿè¸ª
    result = await tracker.track_intent(
        current_message="ç»§ç»­ä¸Šé¢çš„è¯é¢˜",
        conversation_history=[]
    )
    
    assert result.intent_type == "task_continuation"
    assert result.confidence > 0.5

@pytest.mark.asyncio
async def test_quality_evaluation():
    evaluator = ConversationQualityEvaluator()
    
    # æµ‹è¯•è´¨é‡è¯„ä¼°
    report = await evaluator.evaluate_response_quality(
        user_query="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        response="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯..."
    )
    
    assert report.score.relevance > 0.7
    assert len(report.suggestions) >= 0
```

### æ€§èƒ½æµ‹è¯•

```python
# tests/test_performance.py
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

async def test_concurrent_requests():
    """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
    
    async def make_request():
        # æ¨¡æ‹ŸAPIè¯·æ±‚
        start_time = time.time()
        # ... æ‰§è¡Œè¯·æ±‚ ...
        return time.time() - start_time
    
    # å¹¶å‘100ä¸ªè¯·æ±‚
    tasks = [make_request() for _ in range(100)]
    response_times = await asyncio.gather(*tasks)
    
    avg_response_time = sum(response_times) / len(response_times)
    assert avg_response_time < 2.0  # å¹³å‡å“åº”æ—¶é—´å°äº2ç§’
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

1. **å†…å­˜ä¸è¶³**
   ```bash
   # å¢åŠ Dockerå†…å­˜é™åˆ¶
   docker-compose -f docker-compose.enhanced.yml up -d --scale llm-api=2
   ```

2. **æ•°æ®åº“è¿æ¥è¶…æ—¶**
   ```python
   # å¢åŠ è¿æ¥æ± å¤§å°
   SQLALCHEMY_POOL_SIZE=20
   SQLALCHEMY_MAX_OVERFLOW=30
   ```

3. **Redisç¼“å­˜æ»¡äº†**
   ```bash
   # æ¸…ç†Redisç¼“å­˜
   docker exec -it redis redis-cli FLUSHDB
   ```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—
docker-compose -f docker-compose.enhanced.yml logs -f llm-api

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose -f docker-compose.enhanced.yml logs quality-evaluator

# ä½¿ç”¨ELK Stackåˆ†ææ—¥å¿—
curl -X GET "elasticsearch:9200/_search?q=level:ERROR"
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®åº“ä¼˜åŒ–

```sql
-- åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
CREATE INDEX idx_conversations_user_id ON conversations(user_id);
CREATE INDEX idx_messages_conversation_id ON messages(conversation_id);
CREATE INDEX idx_files_user_id ON files(user_id);

-- åˆ†åŒºè¡¨ä¼˜åŒ–
CREATE TABLE conversations_2024 PARTITION OF conversations
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

### 2. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

```python
# æ™ºèƒ½ç¼“å­˜å¤±æ•ˆ
class SmartCache:
    def __init__(self):
        self.cache_stats = {}
        
    def should_cache(self, key: str, content: str) -> bool:
        """æ™ºèƒ½å†³å®šæ˜¯å¦ç¼“å­˜"""
        # åŸºäºå†…å®¹é•¿åº¦ã€è®¿é—®é¢‘ç‡ç­‰å†³å®š
        if len(content) > 10000:  # å¤§å†…å®¹ä¼˜å…ˆç¼“å­˜
            return True
        if self.cache_stats.get(key, 0) > 5:  # é«˜é¢‘è®¿é—®ç¼“å­˜
            return True
        return False
```

### 3. æ¨¡å‹è·¯ç”±ä¼˜åŒ–

```python
# æ™ºèƒ½æ¨¡å‹é€‰æ‹©
class ModelRouter:
    def select_model(self, query: str, context: Dict) -> str:
        """æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
        complexity = self._analyze_complexity(query)
        
        if complexity < 0.3:
            return "gpt-3.5-turbo"  # ç®€å•æŸ¥è¯¢ç”¨è½»é‡æ¨¡å‹
        elif complexity > 0.8:
            return "gpt-4"          # å¤æ‚æŸ¥è¯¢ç”¨é«˜ç«¯æ¨¡å‹
        else:
            return "claude-3-sonnet" # ä¸­ç­‰æŸ¥è¯¢ç”¨å¹³è¡¡æ¨¡å‹
```

## ğŸ¯ æˆåŠŸæŒ‡æ ‡

### å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPIs)

1. **å“åº”æ—¶é—´**: < 2ç§’ (95th percentile)
2. **è´¨é‡è¯„åˆ†**: > 0.8 (å¹³å‡åˆ†)
3. **ç”¨æˆ·æ»¡æ„åº¦**: > 85%
4. **ç³»ç»Ÿå¯ç”¨æ€§**: > 99.9%
5. **é”™è¯¯ç‡**: < 1%

### ç›‘æ§ä»ªè¡¨æ¿

è®¿é—®ä»¥ä¸‹URLæŸ¥çœ‹ç³»ç»ŸçŠ¶æ€ï¼š
- Grafana: http://localhost:3000 (admin/admin123)
- Prometheus: http://localhost:9090
- Kibana: http://localhost:5601
- Jaeger: http://localhost:16686
- Flower: http://localhost:5555

## ğŸ“š å‚è€ƒèµ„æ–™

1. [LangChainå®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
2. [LangGraphæŒ‡å—](https://langchain-ai.github.io/langgraph/)
3. [Docker Composeæœ€ä½³å®è·µ](https://docs.docker.com/compose/production/)
4. [Prometheusç›‘æ§æŒ‡å—](https://prometheus.io/docs/guides/)
5. [Elasticsearchæœç´¢ä¼˜åŒ–](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-optimization.html)

---

é€šè¿‡éµå¾ªè¿™ä¸ªå®æ–½æŒ‡å—ï¼Œæ‚¨å¯ä»¥é€æ­¥å°†LLMç³»ç»Ÿå‡çº§ä¸ºä¼ä¸šçº§çš„æ™ºèƒ½å¯¹è¯å¹³å°ï¼Œå®ç°æ›´å¥½çš„æ€§èƒ½ã€å¯é æ€§å’Œç”¨æˆ·ä½“éªŒã€‚ 